@article{tan2024teola,
  title={Teola: Towards end-to-end optimization of llm-based applications},
  author={Tan, Xin and Jiang, Yimin and Yang, Yitao and Xu, Hong},
  journal={arXiv preprint arXiv:2407.00326},
  year={2024}
}

@misc{llamaindex,
  title = {LlamaIndex},
  year = {2022},
  url = {https://github.com/jerryjliu/llama_index}
}

@misc{promptflow,
  title = {Promptflow},
  year = {2023},
  url = {https://https://github.com/microsoft/promptflow}
}

@misc{autogpt,
  title = {Autogpt},
  year = {2024},
  url = {https://github.com/Significant-Gravitas/AutoGPT}
}

@misc{bing,
  title = {Bing Copilot},
  year = {2024},
  url = {https://www.bing.com/chat}
}

@misc{characterai,
  title = {Character.ai},
  year = {2024},
  url = {https://character.ai/}
}

@misc{contextual-retrieval,
  title = {contextual-retrieval},
  year = {2024},
  url = {https://www.anthropic.com/news/contextual-retrieval}
}

@misc{Fastapi,
  title = {Fastapi},
  year = {2024},
  url = {https://fastapi.tiangolo.com/}
}

@misc{Finqabench,
  title = {Finqabench Dataset},
  year = {2024},
  url = {https://huggingface.co/datasets/lighthouzai/finqabench}
}

@misc{googlesearch,
  title = {Google custom search},
  year = {2024},
  url = {https://programmablesearchengine.google.com/}
}

@misc{azurerag,
  title = {Gpt-rag},
  year = {2024},
  url = {https://github.com/Azure/GPT-RAG}
}

@misc{haystack,
  title = {haystack},
  year = {2024},
  url = {https://github.com/deepset-ai/haystack}
}

@misc{langchain,
  title = {Langchain},
  year = {2024},
  url = {https://github.com/langchain-ai/langchain}
}

@misc{langgraph,
  title = {LangGraph},
  year = {2024},
  url = {https://python.langchain.com/docs/langgraph/}
}

@misc{lazyllm,
  title = {Lazyllm},
  year = {2024},
  url = {https://github.com/LazyAGI/LazyLLM}
}

@misc{alibaba_llmapp_observation,
  title = {Observability of llm applications: Exploration and practice from the perspective of trace},
  year = {2024},
  url = {https://www.alibabacloud.com/blog/observability-of-llm-applications-exploration-and-practice-from-the-perspective-of-trace_601604}
}

@misc{openaifunc,
  title = {Openai function calling},
  year = {2024},
  url = {https://platform.openai.com/docs/guides/function-calling}
}

@misc{pairag,
  title = {Pairag},
  year = {2024},
  url = {https://github.com/aigc-apps/PAI-RAG}
}

@misc{perplexity,
  title = {Perplexity ai},
  year = {2024},
  url = {https://www.perplexity.ai/}
}

@misc{pgvector,
  title = {Pgvector},
  year = {2024},
  url = {https://github.com/pgvector/pgvector}
}

@misc{postgresql,
  title = {Postgresql},
  year = {2024},
  url = {https://www.postgresql.org/}
}

@misc{privatellm,
  title = {Privatellm},
  year = {2024},
  url = {https://privatellm.app/en}
}

@misc{tritonserver,
  title = {Triton inference server},
  year = {2024},
  url = {https://github.com/triton-inference-server}
}

@inproceedings{tensorflow,
  author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek~G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title = {TensorFlow: A system for Large-Scale machine learning},
  booktitle = {Proc.~USENIX OSDI},
  year = {2016}
}

@article{agrawal2024taming,
  author = {Amey Agrawal and Nitin Kedia and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav~S Gulavani and Alexey Tumanov and Ramachandran Ramjee},
  title = {Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.02310}
}

@article{agrawal2023sarathi,
  author = {Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav~S Gulavani and Ramachandran Ramjee},
  title = {Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.16369}
}

@inproceedings{bang2023gptcache,
  author = {Fu~Bang},
  title = {Gptcache: An open-source semantic cache for llm applications enabling faster answers and cost savings},
  booktitle = {Proc.~the 3rd Workshop for Natural Language Processing Open Source Software},
  year = {2023}
}

@inproceedings{bauer2012legion,
  author = {Michael Bauer and Sean Treichler and Elliott Slaughter and Alex Aiken},
  title = {Legion: Expressing locality and independence with logical regions},
  booktitle = {Proc. IEEE SC},
  year = {2012}
}

@inproceedings{webquestion,
  author = {Jonathan Berant and Andrew Chou and Roy Frostig and Percy Liang},
  title = {Semantic parsing on Freebase from question-answer pairs},
  booktitle = {Proc.~EMNLP},
  year = {2013},
  month = {October}
}

@inproceedings{crankshaw2017clipper,
  author = {Daniel Crankshaw and Xin Wang and Guilio Zhou and Michael~J Franklin and Joseph~E Gonzalez and Ion Stoica},
  title = {Clipper: A Low-Latency online prediction serving system},
  booktitle = {Proc.~USENIX NSDI},
  year = {2017}
}

@inproceedings{dao2022flashattention,
  author = {Tri Dao and Dan Fu and Stefano Ermon and Atri Rudra and Christopher R{\'e}},
  title = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  booktitle = {Proc.~NeurIPS},
  year = {2022}
}

@inproceedings{devlin2018bert,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  booktitle = {Proc.~ACL},
  year = {2018}
}

@article{gao2023retrievalsurvey,
  author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi~Dai and Jiawei Sun and Haofen Wang},
  title = {Retrieval-augmented generation for large language models: A survey},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.10997}
}

@article{gim2023promptcache,
  author = {In~Gim and Guojun Chen and Seung-seob Lee and Nikhil Sarda and Anurag Khandelwal and Lin Zhong},
  title = {Prompt cache: Modular attention reuse for low-latency inference},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.04934}
}

@inproceedings{gog2015musketeer,
  author = {Ionel Gog and Malte Schwarzkopf and Natacha Crooks and Matthew~P Grosvenor and Allen Clement and Steven Hand},
  title = {Musketeer: all for one, one for all in data processing systems},
  booktitle = {Proc. ACM Eurosys},
  year = {2015}
}

@inproceedings{gujarati2020clockwork,
  author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
  title = {Serving DNNs like clockwork: Performance predictability from the bottom up},
  booktitle = {Proc.~USENIX OSDI},
  year = {2020}
}

@inproceedings{hong2023flashdecoding++,
  author = {Ke~Hong and Guohao Dai and Jiaming Xu and Qiuli Mao and Xiuhong Li and Jun Liu and Kangdi Chen and Hanyu Dong and Yu~Wang},
  title = {Flashdecoding++: Faster large language model inference on gpus},
  booktitle = {Proc.~Machine Learning and Systems},
  year = {2023}
}

@article{hong2024data,
  author = {Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li~Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},
  title = {Data interpreter: An llm agent for data science},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2402.18679}
}

@article{hong2023metagpt,
  author = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka~Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
  title = {Metagpt: Meta programming for a multi-agent collaborative framework},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.00352}
}

@article{hu2024inferencewithoutinterference,
  author = {Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa~Wang and Yungang Bao and et~al.},
  title = {Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.11181}
}

@article{huang2023Hallucination,
  author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
  title = {A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.05232}
}

@article{huang2024tool,
  author = {Zhongzhen Huang and Kui Xue and Yongqi Fan and Linjie Mu and Ruoyu Liu and Tong Ruan and Shaoting Zhang and Xiaofan Zhang},
  title = {Tool calling: Enhancing medication consultation via retrieval-augmented large language models},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.17897}
}

@inproceedings{isard2007dryad,
  author = {Michael Isard and Mihai Budiu and Yuan Yu and Andrew Birrell and Dennis Fetterly},
  title = {Dryad: distributed data-parallel programs from sequential building blocks},
  booktitle = {Proc.~ACM Eurosys},
  year = {2007}
}

@article{jagerman2023queryexpansion,
  author = {Rolf Jagerman and Honglei Zhuang and Zhen Qin and Xuanhui Wang and Michael Bendersky},
  title = {Query expansion by prompting large language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2305.03653}
}

@article{jeong2024adaptiverag,
  author = {Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung~Ju Hwang and Jong~C Park},
  title = {Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.14403}
}

@article{jin2024ragcache,
  author = {Chao Jin and Zili Zhang and Xuanlin Jiang and Fangyue Liu and Xin Liu and Xuanzhe Liu and Xin Jin},
  title = {Ragcache: Efficient knowledge caching for retrieval-augmented generation},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.12457}
}

@article{khattab2023dspy,
  author = {Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan and Saiful Haq and Ashutosh Sharma and Thomas~T Joshi and Hanna Moazam and et~al.},
  title = {Dspy: Compiling declarative language model calls into self-improving pipelines},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2310.03714}
}

@article{kim2023llmcompiler,
  author = {Sehoon Kim and Suhong Moon and Ryan Tabrizi and Nicholas Lee and Michael~W Mahoney and Kurt Keutzer and Amir Gholami},
  title = {An llm compiler for parallel function calling},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.04511}
}

@inproceedings{kwon2023vllm,
  author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody~Hao Yu and Joseph Gonzalez and Hao Zhang and Ion Stoica},
  title = {Efficient memory management for large language model serving with pagedattention},
  booktitle = {Proc.~ACM SOSP},
  year = {2023}
}

@article{lewis2020retrieval,
  author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{\"u}ttler and Mike Lewis and Wen-tau Yih and Tim Rockt{\"a}schel and et~al.},
  title = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  journal = {arXiv},
  year = {2020}
}

@inproceedings{li2023alpaserve,
  author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph~E Gonzalez and et~al.},
  title = {AlpaServe: Statistical multiplexing with model parallelism for deep learning serving},
  booktitle = {Proc.~USENIX OSDI},
  year = {2023}
}

@article{lin2024infinite,
  author = {Bin Lin and Tao Peng and Chen Zhang and Minmin Sun and Lanbo Li and Hanyu Zhao and Wencong Xiao and Qi~Xu and Xiafei Qiu and Shen Li and et~al.},
  title = {Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.02669}
}

@inproceedings{lin2024parrot,
  author = {Chaofan Lin and Zhenhua Han and Chengruidong Zhang and Yuqing Yang and Fan Yang and Chen Chen and Lili Qiu},
  title = {Parrot: Efficient serving of llm-based applications with semantic variable},
  booktitle = {Proc.~USENIX OSDI},
  year = {2024}
}

@article{lin2021truthfulqa,
  author = {Stephanie Lin and Jacob Hilton and Owain Evans},
  title = {Truthfulqa: Measuring how models mimic human falsehoods},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2109.07958}
}

@article{liu2024optimizingquery,
  author = {Shu Liu and Asim Biswal and Audrey Cheng and Xiangxi Mo and Shiyi Cao and Joseph~E Gonzalez and Ion Stoica and Matei Zaharia},
  title = {Optimizing llm queries in relational workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05821}
}

@article{liu2024optimrelation,
  author = {Shu Liu and Asim Biswal and Audrey Cheng and Xiangxi Mo and Shiyi Cao and Joseph~E Gonzalez and Ion Stoica and Matei Zaharia},
  title = {Optimizing llm queries in relational workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05821}
}

@article{liu2023onlinespec,
  author = {Xiaoxuan Liu and Lanxiang Hu and Peter Bailis and Ion Stoica and Zhijie Deng and Alvin Cheung and Hao Zhang},
  title = {Online speculative decoding},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2310.07177}
}

@article{liu2024iterativere,
  author = {Yanming Liu and Xinyue Peng and Xuhong Zhang and Weihao Liu and Jianwei Yin and Jiannan Cao and Tianyu Du},
  title = {Ra-isf: Learning to answer and understand from retrieval augmentation via iterative self-feedback},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.06840}
}

@inproceedings{madaan2024selfreflection,
  author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and et~al.},
  title = {Self-refine: Iterative refinement with self-feedback},
  booktitle = {Proc.~NeurIPS},
  year = {2024}
}

@inproceedings{miao2024specinfer,
  author = {Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Zhengxin Zhang and Rae Ying~Yee Wong and Alan Zhu and Lijie Yang and Xiaoxiang Shi and et~al.},
  title = {Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  booktitle = {Proc.~ACM ASPLOS},
  year = {2024}
}

@inproceedings{miao2023spotserve,
  author = {Xupeng Miao and Chunan Shi and Jiangfei Duan and Xiaoli Xi and Dahua Lin and Bin Cui and Zhihao Jia},
  title = {Spotserve: Serving generative large language models on preemptible instances},
  booktitle = {Proc.~ACM ASPLOS},
  year = {2024}
}

@inproceedings{moritz2018ray,
  author = {Philipp Moritz and Robert Nishihara and Stephanie Wang and Alexey Tumanov and Richard Liaw and Eric Liang and Melih Elibol and Zongheng Yang and William Paul and Michael~I Jordan and et~al.},
  title = {Ray: A distributed framework for emerging AI applications},
  booktitle = {Proc.~USENIX OSDI},
  year = {2018}
}

@article{ou2024losslessdecoding,
  author = {Jie Ou and Yueming Chen and Wenhong Tian},
  title = {Lossless acceleration of large language model via adaptive n-gram parallel decoding},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.08698}
}

@article{patel2023splitwise,
  author = {Pratyush Patel and Esha Choukse and Chaojie Zhang and {\'I}{\~n}igo Goiri and Aashaka Shah and Saeed Maleki and Ricardo Bianchini},
  title = {Splitwise: Efficient generative llm inference using phase splitting},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.18677}
}

@inproceedings{shen2023hugginggpt,
  author = {Yongliang Shen and Kaitao Song and Xu~Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
  title = {Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  booktitle = {Proc.~NeurIPS},
  year = {2023}
}

@article{sheng2023fairness,
  author = {Ying Sheng and Shiyi Cao and Dacheng Li and Banghua Zhu and Zhuohan Li and Danyang Zhuo and Joseph~E Gonzalez and Ion Stoica},
  title = {Fairness in serving large language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2401.00588}
}

@article{tan2024small,
  author = {Jiejun Tan and Zhicheng Dou and Yutao Zhu and Peidong Guo and Kun Fang and Ji-Rong Wen},
  title = {Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2402.12052}
}

@article{team2024gemma,
  author = {Gemma Team and Morgane Riviere and Shreya Pathak and Pier~Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and L{\'e}onard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ram{\'e} and et~al.},
  title = {Gemma 2: Improving open language models at a practical size},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2408.00118}
}

@article{touvron2023llamamodel,
  author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and et~al.},
  title = {Llama: Open and efficient foundation language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2302.13971}
}

@article{touvron2023llama2model,
  author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and et~al.},
  title = {Llama 2: Open foundation and fine-tuned chat models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2307.09288}
}

@inproceedings{attention,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan~N Gomez and \L~ukasz Kaiser and Illia Polosukhin},
  title = {Attention is All you Need},
  booktitle = {Proc.~NeurIPS},
  year = {2017}
}

@article{wu2024loongserve,
  author = {Bingyang Wu and Shengyu Liu and Yinmin Zhong and Peng Sun and Xuanzhe Liu and Xin Jin},
  title = {Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.09526}
}

@article{wu2023autogen,
  author = {Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li~Jiang and Xiaoyun Zhang and Chi Wang},
  title = {Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.08155}
}

@inproceedings{xiao2023smoothquant,
  author = {Guangxuan Xiao and Ji~Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
  title = {Smoothquant: Accurate and efficient post-training quantization for large language models},
  booktitle = {Proc.~ICML},
  year = {2023}
}

@article{xiao2023bgeembedding,
  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighof},
  title = {C-pack: Packaged resources to advance general chinese embedding},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2309.07597}
}

@inproceedings{yang2018hotpotqa,
  author = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William~W. Cohen and Ruslan Salakhutdinov and Christopher~D. Manning},
  title = {HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  booktitle = {Proc.~EMNLP},
  year = {2018}
}

@inproceedings{yu2022orca,
  author = {Gyeong-In Yu and Joo~Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
  title = {Orca: A distributed serving system for Transformer-Based generative models},
  booktitle = {Proc.~USENIX OSDI},
  year = {2022}
}

@article{zaharia2016apachespark,
  author = {Matei Zaharia and Reynold~S Xin and Patrick Wendell and Tathagata Das and Michael Armbrust and Ankur Dave and Xiangrui Meng and Josh Rosen and Shivaram Venkataraman and Michael~J Franklin and et~al.},
  title = {Apache spark: a unified engine for big data processing},
  journal = {Communications of the ACM},
  year = {2016}
}

@inproceedings{zhang2023shepherd,
  author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
  title = {SHEPHERD: Serving DNNs in the wild},
  booktitle = {Proc.~USENIX NSDI},
  year = {2023}
}

@article{zheng2023sglang,
  author = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody~Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph~E Gonzalez and et~al.},
  title = {Efficiently programming large language models using sglang},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.07104}
}

@article{zhong2024distserve,
  author = {Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
  title = {Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.09670}
}

@article{zhu2023optimalcache,
  author = {Banghua Zhu and Ying Sheng and Lianmin Zheng and Clark Barrett and Michael~I Jordan and Jiantao Jiao},
  title = {On optimal caching and model multiplexing for large model inference},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2306.02003}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
@article{shah2024flashattention,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}
@misc{tensorrtllm, title={TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference}, url={https://github.com/NVIDIA/TensorRT-LLM}, journal={Nvidia}}
@article{liu2024andes,
  title={Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services},
  author={Liu, Jiachen and Wu, Zhiyu and Chung, Jae-Won and Lai, Fan and Lee, Myungjin and Chowdhury, Mosharaf},
  journal={arXiv preprint arXiv:2404.16283},
  year={2024}
}
@misc{tgi, title={Text Generation Inference: A Rust, Python and gRPC server for text generation inference.}, url={https://github.com/huggingface/text-generation-inference}, journal={HuggingFace}}

@article{zheng2024response,
  title={Response length perception and sequence scheduling: An llm-empowered llm inference pipeline},
  author={Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{hong2024flashdecoding++,
  title={FlashDecoding++: Faster Large Language Model Inference with Asynchronization, Flat GEMM Optimization, and Heuristics},
  author={Hong, Ke and Dai, Guohao and Xu, Jiaming and Mao, Qiuli and Li, Xiuhong and Liu, Jun and Dong, Yuhan and Wang, Yu and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={148--161},
  year={2024}
}
@inproceedings {298503,
author = {Kinman Lei and Yuyang Jin and Mingshu Zhai and Kezhao Huang and Haoxing Ye and Jidong Zhai},
title = {{PUZZLE}: Efficiently Aligning Large Language Models through {Light-Weight} Context Switch},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {127--140},
url = {https://www.usenix.org/conference/atc24/presentation/lei},
publisher = {USENIX Association},
month = jul
}
@article{sheng2023fairness,
  title={Fairness in serving large language models},
  author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Zhu, Banghua and Li, Zhuohan and Zhuo, Danyang and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2401.00588},
  year={2023}
}
@article{li2024toward,
  title={Toward sustainable genai using generation directives for carbon-friendly large language model inference},
  author={Li, Baolin and Jiang, Yankai and Gadepally, Vijay and Tiwari, Devesh},
  journal={arXiv preprint arXiv:2403.12900},
  year={2024}
}
@article{huang2023towards,
  title={Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference},
  author={Huang, Haiyang and Ardalani, Newsha and Sun, Anna and Ke, Liu and Lee, Hsien-Hsin S and Sridhar, Anjali and Bhosale, Shruti and Wu, Carole-Jean and Lee, Benjamin},
  journal={arXiv preprint arXiv:2303.06182},
  year={2023}
}
@article{kamahori2024fiddler,
  title={Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models},
  author={Kamahori, Keisuke and Gu, Yile and Zhu, Kan and Kasikci, Baris},
  journal={arXiv preprint arXiv:2402.07033},
  year={2024}
}
@article{xue2024moe,
  title={Moe-infinity: Activation-aware expert offloading for efficient moe serving},
  author={Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh},
  journal={arXiv preprint arXiv:2401.14361},
  year={2024}
}
@article{du2024sida,
  title={SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models},
  author={Du, Zhixu and Li, Shiyu and Wu, Yuhao and Jiang, Xiangyu and Sun, Jingwei and Zheng, Qilin and Wu, Yongkai and Li, Ang and Li, Hai and Chen, Yiran},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={224--238},
  year={2024}
}
@inproceedings{yao2024exploiting,
  title={Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference},
  author={Yao, Jinghan and Anthony, Quentin and Shafi, Aamir and Subramoni, Hari and Panda, Dhabaleswar K DK},
  booktitle={2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={915--925},
  year={2024},
  organization={IEEE}
}
@inproceedings{li2023accelerating,
  title={Accelerating distributed $\{$MoE$\}$ training and inference with lina},
  author={Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={945--959},
  year={2023}
}
@article{xuanlei2024hetegen,
  title={HeteGen: Efficient Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices},
  author={XUANLEI, ZHAO and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={162--172},
  year={2024}
}
@article{miao2024flexllm,
  title={FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning},
  author={Miao, Xupeng and Oliaro, Gabriele and Cheng, Xinhao and Wu, Mengdi and Unger, Colin and Jia, Zhihao},
  journal={arXiv preprint arXiv:2402.18789},
  year={2024}
}
@article{yang2024perllm,
  title={PerLLM: Personalized Inference Scheduling with Edge-Cloud Collaboration for Diverse LLM Services},
  author={Yang, Zheming and Yang, Yuanhao and Zhao, Chang and Guo, Qi and He, Wenkai and Ji, Wen},
  journal={arXiv preprint arXiv:2405.14636},
  year={2024}
}
@inproceedings{patel2024characterizing,
  title={Characterizing Power Management Opportunities for LLMs in the Cloud},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Warrier, Brijesh and Mahalingam, Nithish and Bianchini, Ricardo},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={207--222},
  year={2024}
}
@article{fu2024serverlessllm,
  title={ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models},
  author={Fu, Yao and Xue, Leyang and Huang, Yeqi and Brabete, Andrei-Octavian and Ustiugov, Dmitrii and Patel, Yuvraj and Mai, Luo},
  journal={arXiv preprint arXiv:2401.14351},
  year={2024}
}
@article{sun2024llumnix,
  title={Llumnix: Dynamic Scheduling for Large Language Model Serving},
  author={Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei},
  journal={arXiv preprint arXiv:2406.03243},
  year={2024}
}
@article{griggs2024m,
  title={M$\backslash$'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity},
  author={Griggs, Tyler and Liu, Xiaoxuan and Yu, Jiaxiang and Kim, Doyoung and Chiang, Wei-Lin and Cheung, Alvin and Stoica, Ion},
  journal={arXiv preprint arXiv:2404.14527},
  year={2024}
}
@inproceedings{miao2024spotserve,
  title={Spotserve: Serving generative large language models on preemptible instances},
  author={Miao, Xupeng and Shi, Chunan and Duan, Jiangfei and Xi, Xiaoli and Lin, Dahua and Cui, Bin and Jia, Zhihao},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={1112--1127},
  year={2024}
}
@article{mei2024helix,
  title={Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs},
  author={Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi},
  journal={arXiv preprint arXiv:2406.01566},
  year={2024}
}
@article{zhong2024distserve,
  title={Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={arXiv preprint arXiv:2401.09670},
  year={2024}
}
@inproceedings{oh2024exegpt,
  title={Exegpt: Constraint-aware resource scheduling for llm inference},
  author={Oh, Hyungjun and Kim, Kihong and Kim, Jaemin and Kim, Sungkyun and Lee, Junyeol and Chang, Du-seong and Seo, Jiwon},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={369--384},
  year={2024}
}
@article{patel2023splitwise,
  title={Splitwise: Efficient generative llm inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Shah, Aashaka and Maleki, Saeed and Bianchini, Ricardo},
  journal={arXiv preprint arXiv:2311.18677},
  year={2023}
}
@article{hu2024inference,
  title={Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
  journal={arXiv preprint arXiv:2401.11181},
  year={2024}
}

@article{agrawal2024taming,
  title={Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2403.02310},
  year={2024}
}
@article{holmes2024deepspeed,
  title={Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference},
  author={Holmes, Connor and Tanaka, Masahiro and Wyatt, Michael and Awan, Ammar Ahmad and Rasley, Jeff and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Qin, Heyang and Bakhtiari, Arash and Kurilenko, Lev and others},
  journal={arXiv preprint arXiv:2401.08671},
  year={2024}
}
@article{jin2023s,
  title={S3: Increasing GPU Utilization during Generative Inference for Higher Throughput},
  author={Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={18015--18027},
  year={2023}
}

@article{prabhu2024vattention,
  title={vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention},
  author={Prabhu, Ramya and Nayak, Ajay and Mohan, Jayashree and Ramjee, Ramachandran and Panwar, Ashish},
  journal={arXiv preprint arXiv:2405.04437},
  year={2024}
}

@article{gim2024prompt,
  title={Prompt cache: Modular attention reuse for low-latency inference},
  author={Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={325--338},
  year={2024}
}

@inproceedings {gaocost,
author = {Bin Gao and Zhuomin He and Puru Sharma and Qingxuan Kang and Djordje Jevdjic and Junbo Deng and Xingkun Yang and Zhou Yu and Pengfei Zuo},
title = {{Cost-Efficient} Large Language Model Serving for Multi-turn Conversations with {CachedAttention}},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {111--126},
url = {https://www.usenix.org/conference/atc24/presentation/gao-bin-cost},
publisher = {USENIX Association},
month = jul
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}
@article{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}
@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}
@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}
@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}
@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{lin2024infinite,
  title={Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
  author={Lin, Bin and Peng, Tao and Zhang, Chen and Sun, Minmin and Li, Lanbo and Zhao, Hanyu and Xiao, Wencong and Xu, Qi and Qiu, Xiafei and Li, Shen and others},
  journal={arXiv preprint arXiv:2401.02669},
  year={2024}
}
@article{hu2024memserve,
  title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool},
  author={Hu, Cunchen and Huang, Heyang and Hu, Junhao and Xu, Jiang and Chen, Xusheng and Xie, Tao and Wang, Chenxi and Wang, Sa and Bao, Yungang and Sun, Ninghui and others},
  journal={arXiv preprint arXiv:2406.17565},
  year={2024}
}
@article{lee2024infinigen,
  title={InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  journal={arXiv preprint arXiv:2406.19707},
  year={2024}
}

@article{wu2024loongserve,
  title={LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism},
  author={Wu, Bingyang and Liu, Shengyu and Zhong, Yinmin and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.09526},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={606--624},
  year={2023}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@article{yuan2024llm,
  title={Llm inference unveiled: Survey and roofline model insights},
  author={Yuan, Zhihang and Shang, Yuzhang and Zhou, Yang and Dong, Zhen and Xue, Chenhao and Wu, Bingzhe and Li, Zhikai and Gu, Qingyi and Lee, Yong Jae and Yan, Yan and others},
  journal={arXiv preprint arXiv:2402.16363},
  year={2024}
}
@article{miao2023towards,
  title={Towards efficient generative large language model serving: A survey from algorithms to systems},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  journal={arXiv preprint arXiv:2312.15234},
  year={2023}
}
@article{zhou2024survey,
  title={A survey on efficient inference for large language models},
  author={Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others},
  journal={arXiv preprint arXiv:2404.14294},
  year={2024}
}
@article{adnan2024keyformer,
  title={Keyformer: Kv cache reduction through key tokens selection for efficient generative inference},
  author={Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={114--127},
  year={2024}
}
@article{fu2024break,
  title={Break the sequential dependency of llm inference using lookahead decoding},
  author={Fu, Yichao and Bailis, Peter and Stoica, Ion and Zhang, Hao},
  journal={arXiv preprint arXiv:2402.02057},
  year={2024}
}
@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}
@article{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}
@article{jin2024ragcache,
  title={RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation},
  author={Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.12457},
  year={2024}
}
@article{zhu2024accelerating,
  title={Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection},
  author={Zhu, Yun and Gu, Jia-Chen and Sikora, Caitlin and Ko, Ho and Liu, Yinxiao and Lin, Chu-Cheng and Shu, Lei and Luo, Liangchen and Meng, Lei and Liu, Bang and others},
  journal={arXiv preprint arXiv:2405.16178},
  year={2024}
}
@article{ong2024routellm,
  title={RouteLLM: Learning to Route LLMs with Preference Data},
  author={Ong, Isaac and Almahairi, Amjad and Wu, Vincent and Chiang, Wei-Lin and Wu, Tianhao and Gonzalez, Joseph E and Kadous, M Waleed and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.18665},
  year={2024}
}

@inproceedings{miao2024specinfer,
  title={Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={932--949},
  year={2024}
}

@article{lin2024parrot,
  title={Parrot: Efficient Serving of LLM-based Applications with Semantic Variable},
  author={Lin, Chaofan and Han, Zhenhua and Zhang, Chengruidong and Yang, Yuqing and Yang, Fan and Chen, Chen and Qiu, Lili},
  journal={arXiv preprint arXiv:2405.19888},
  year={2024}
}

@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}



@inproceedings{FrantarA23,
  author       = {Elias Frantar and
                  Dan Alistarh},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {SparseGPT: Massive Language Models Can be Accurately Pruned in One-Shot},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {10323--10337},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/frantar23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/FrantarA23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{sun2024a,
title={A Simple and Effective Pruning Approach for Large Language Models},
author={Mingjie Sun and Zhuang Liu and Anna Bair and J Zico Kolter},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=PxoFut3dWW}
}


@INPROCEEDINGS{10445737,
  author={Shao, Hang and Liu, Bei and Qian, Yanmin},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={11296-11300},
  keywords={Degradation;Quantization (signal);Sensitivity;Transformers;Resource management;Task analysis;Speech processing;model compression;sparsity pruning;large language models;mixed sparsity},
  doi={10.1109/ICASSP48485.2024.10445737}
  }


@inproceedings{
zhang2024dynamic,
title={Dynamic Sparse No Training:  Training-Free Fine-tuning for Sparse {LLM}s},
author={Yuxin Zhang and Lirui Zhao and Mingbao Lin and Sun Yunyun and Yiwu Yao and Xingjia Han and Jared Tanner and Shiwei Liu and Rongrong Ji},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=1ndDmZdT4g}
}

@inproceedings{dong2024pruner,
  title={Pruner-Zero: Evolving Symbolic Pruning Metric from scratch for Large Language Models},
  author={Dong, Peijie and Li, Lujun and Tang, Zhenheng and Liu, Xiang and Pan, Xinglin and Wang, Qiang and Chu, Xiaowen},
  booktitle={Forty-first International Conference on Machine Learning}
}

@misc{dong2024stbllmbreaking1bitbarrier,
      title={STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs}, 
      author={Peijie Dong and Lujun Li and Yuedong Zhong and Dayou Du and Ruibo Fan and Yuhan Chen and Zhenheng Tang and Qiang Wang and Wei Xue and Yike Guo and Xiaowen Chu},
      year={2024},
      eprint={2408.01803},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.01803}, 
}


@article{tang2020survey,
	title        = {Communication-efficient distributed deep learning: A comprehensive survey},
	author       = {Tang, Zhenheng and Shi, Shaohuai and Chu, Xiaowen and Wang, Wei and Li, Bo},
	journal      = {arXiv preprint arXiv:2003.06307},
	year         = {2020}
}


@INPROCEEDINGS{10445737,
  author={Shao, Hang and Liu, Bei and Qian, Yanmin},
  booktitle={ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models}, 
  year={2024},
  volume={},
  number={},
  pages={11296-11300},
  keywords={Degradation;Quantization (signal);Sensitivity;Transformers;Resource management;Task analysis;Speech processing;model compression;sparsity pruning;large language models;mixed sparsity},
  doi={10.1109/ICASSP48485.2024.10445737}
  }


@inproceedings{MolchanovMTFK19,
  author       = {Pavlo Molchanov and
                  Arun Mallya and
                  Stephen Tyree and
                  Iuri Frosio and
                  Jan Kautz},
  title        = {Importance Estimation for Neural Network Pruning},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2019, Long Beach, CA, USA, June 16-20, 2019},
  pages        = {11264--11272},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2019},
  url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Molchanov\_Importance\_Estimation\_for\_Neural\_Network\_Pruning\_CVPR\_2019\_paper.html},
  doi          = {10.1109/CVPR.2019.01152},
  timestamp    = {Tue, 11 Jul 2023 08:22:02 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/MolchanovMTFK19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{NIPS2015_ae0eb3ee,
 author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Learning both Weights and Connections for Efficient Neural Network},
 url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/ae0eb3eed39d2bcef4622b2499a05fe6-Paper.pdf},
 volume = {28},
 year = {2015}
}


@inproceedings{
park2024lutgemm,
title={{LUT}-{GEMM}: Quantized Matrix Multiplication based on {LUT}s for Efficient Inference in Large-Scale Generative Language Models},
author={Gunho Park and Baeseong park and Minsub Kim and Sungjae Lee and Jeonghoon Kim and Beomseok Kwon and Se Jung Kwon and Byeongwook Kim and Youngjoo Lee and Dongsoo Lee},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=gLARhFLE0F}
}


@inproceedings{
frantar2022optimal,
title={Optimal Brain Compression: A Framework for Accurate Post-Training Quantization and Pruning},
author={Elias Frantar and Dan Alistarh},
booktitle={Advances in Neural Information Processing Systems},
editor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},
year={2022},
url={https://openreview.net/forum?id=ksVGCOlOEba}
}


@inproceedings{LeeJKKP24,
  author       = {Changhun Lee and
                  Jungyu Jin and
                  Taesu Kim and
                  Hyungjun Kim and
                  Eunhyeok Park},
  editor       = {Michael J. Wooldridge and
                  Jennifer G. Dy and
                  Sriraam Natarajan},
  title        = {{OWQ:} Outlier-Aware Weight Quantization for Efficient Fine-Tuning
                  and Inference of Large Language Models},
  booktitle    = {Thirty-Eighth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2024, Thirty-Sixth Conference on Innovative Applications of Artificial
                  Intelligence, {IAAI} 2024, Fourteenth Symposium on Educational Advances
                  in Artificial Intelligence, {EAAI} 2014, February 20-27, 2024, Vancouver,
                  Canada},
  pages        = {13355--13364},
  publisher    = {{AAAI} Press},
  year         = {2024},
  url          = {https://doi.org/10.1609/aaai.v38i12.29237},
  doi          = {10.1609/AAAI.V38I12.29237},
  timestamp    = {Wed, 27 Mar 2024 16:01:52 +0100},
  biburl       = {https://dblp.org/rec/conf/aaai/LeeJKKP24.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{abs-2306-07629,
  author       = {Sehoon Kim and
                  Coleman Hooper and
                  Amir Gholami and
                  Zhen Dong and
                  Xiuyu Li and
                  Sheng Shen and
                  Michael W. Mahoney and
                  Kurt Keutzer},
  title        = {SqueezeLLM: Dense-and-Sparse Quantization},
  journal      = {CoRR},
  volume       = {abs/2306.07629},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.07629},
  doi          = {10.48550/ARXIV.2306.07629},
  eprinttype    = {arXiv},
  eprint       = {2306.07629},
  timestamp    = {Sat, 17 Jun 2023 18:52:05 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-07629.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@article{abs-2306-00978,
  author       = {Ji Lin and
                  Jiaming Tang and
                  Haotian Tang and
                  Shang Yang and
                  Xingyu Dang and
                  Song Han},
  title        = {{AWQ:} Activation-aware Weight Quantization for {LLM} Compression
                  and Acceleration},
  journal      = {CoRR},
  volume       = {abs/2306.00978},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2306.00978},
  doi          = {10.48550/arXiv.2306.00978},
  eprinttype    = {arXiv},
  eprint       = {2306.00978},
  timestamp    = {Mon, 12 Jun 2023 16:25:59 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2306-00978.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{DongYGMK19,
  author       = {Zhen Dong and
                  Zhewei Yao and
                  Amir Gholami and
                  Michael W. Mahoney and
                  Kurt Keutzer},
  title        = {{HAWQ:} Hessian AWare Quantization of Neural Networks With Mixed-Precision},
  booktitle    = {2019 {IEEE/CVF} International Conference on Computer Vision, {ICCV}
                  2019, Seoul, Korea (South), October 27 - November 2, 2019},
  pages        = {293--302},
  publisher    = {{IEEE}},
  year         = {2019},
  url          = {https://doi.org/10.1109/ICCV.2019.00038},
  doi          = {10.1109/ICCV.2019.00038},
  timestamp    = {Thu, 05 Mar 2020 13:43:22 +0100},
  biburl       = {https://dblp.org/rec/conf/iccv/DongYGMK19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}







@misc{packing,
    title = {NVIDIA Effective Transformer},
    year = {2020},
    howpublished = {\url{https://github.com/bytedance/effective_transformer}},
    note = {Commit: e406421, Accessed on: 2023-11-25}
}

@misc{fastertransformer,
    title = {NVIDIA FasterTransformer},
    year = {2021},
    howpublished = {\url{https://github.com/NVIDIA/FasterTransformer}},
    note = {Commit: df4a753, Accessed on: 2023-11-25}
}

@misc{dsinfer,
    title = {DeepSpeed Inference},
    year = {2022},
    howpublished = {\url{https://github.com/microsoft/DeepSpeed}},
    note = {Commit: 2afa1c7, Accessed on: 2023-11-25}
}

@misc{nvhopper,
    title = {NVIDIA H100 Tensor Core GPU Architecture},
    year = {2022},
    howpublished = {\url{https://resources.nvidia.com/en-us-tensor-core/gtc22-whitepaper-hopper}},
    note = {Accessed on: 2023-11-25}
}

@misc{llmperf,
    title = {AnyScale LLMPerf leaderboard},
    year = {2023},
    howpublished = {\url{https://github.com/ray-project/llmperf-leaderboard}},
    note = {Accessed on: 2023-12-23}
}

@misc{awsinfer,
    title = {AWS Inferentia},
    year = {2023},
    howpublished = {\url{https://aws.amazon.com/blogs/machine-learning/deploy-large-language-models-on-aws-inferentia2-using-large-model-inference-containers/}}
}

@misc{chatglm2,
    title = {ChatGLM2-6B},
    year = {2023},
    howpublished = {\url{https://huggingface.co/THUDM/chatglm2-6b}}
}

@misc{ctrans,
    title = {CTranslate2},
    year = {2023},
    howpublished = {\url{https://github.com/OpenNMT/CTranslate2}},
    note = {Commit: d963499, Accessed on: 2023-11-25}
}

@misc{deepspeedfastgen,
    title = {DeepSpeed-FastGen},
    year = {2023},
    howpublished = {\url{https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fastgen}},
    note = {Accessed on: 2023-11-25}
}

@misc{dszeroinfer,
    title = {DeepSpeed-Inference v.s. ZeRO-Inference},
    year = {2023},
    howpublished = {\url{https://github.com/microsoft/DeepSpeed/issues/4234}},
    note = {Accessed on: 2023-11-25}
}

@misc{deepspeedmii,
    title = {DeepSpeed-MII},
    year = {2023},
    howpublished = {\url{https://github.com/microsoft/DeepSpeed-MII}},
    note = {Commit: f34b772, Accessed on: 2023-11-25}
}

@misc{flexflowserve,
    title = {FlexFlow-Serve},
    year = {2023},
    howpublished = {\url{https://github.com/Flexflow/FlexFlow/tree/inference}},
    note = {Commit: 672cdad, Accessed on: 2023-11-25}
}

@misc{flexgen,
    title = {FlexGen},
    year = {2023},
    howpublished = {\url{https://github.com/FMInference/FlexGen}},
    note = {Commit: d34f7b4, Accessed on: 2023-11-25}
}

@misc{ggml,
    title = {ggml},
    year = {2023},
    howpublished = {\url{https://github.com/ggerganov/ggml}},
    note = {Commit: a5e4560, Accessed on: 2023-11-25}
}

@misc{pytorchgptfast,
    title = {gpt-fast},
    year = {2023},
    howpublished = {\url{https://github.com/pytorch-labs/gpt-fast}},
    note = {Commit: 8c8c463, Accessed on: 2023-12-23}
}

@misc{graphcore,
    title = {Graphcore},
    year = {2023},
    howpublished = {\url{https://www.graphcore.ai/posts/dolly-2.0-open-source-language-model-with-chatgpt-like-interactivity}}
}

@misc{poptrans,
    title = {Graphcore PopTransformer},
    year = {2023},
    howpublished = {\url{https://github.com/graphcore/PopTransformer}},
    note = {Commit: 1314598, Accessed on: 2023-11-25}
}

@misc{tgi,
    title = {Huggingface Text Generation Inference},
    year = {2023},
    howpublished = {\url{https://github.com/huggingface/text-generation-inference}},
    note = {Commit: 3c02262, Accessed on: 2023-11-25}
}

@misc{intelcpu,
    title = {Intel Extension for Transformers},
    year = {2023},
    howpublished = {\url{https://github.com/intel/intel-extension-for-transformers}},
    note = {Commit: 37d4007, Accessed on: 2023-12-23}
}

@misc{lmdeploy,
    title = {InterLM LMDeploy},
    year = {2023},
    howpublished = {\url{https://github.com/InternLM/lmdeploy}},
    note = {Commit: c07f60f, Accessed on: 2023-11-25}
}

@misc{lightllm,
    title = {LightLLM},
    year = {2023},
    howpublished = {\url{https://github.com/ModelTC/lightllm}},
    note = {Commit: 84671a7, Accessed on: 2023-11-25}
}

@misc{hamel,
    title = {Llama-v2-7b benchmark},
    year = {2023},
    howpublished = {\url{https://hamel.dev/notes/llm/inference/03_inference.html}},
    note = {Accessed on: 2023-11-25}
}

@misc{cudnnmha,
    title = {NVIDIA cuDNN MultiHeadAttn},
    year = {2023},
    howpublished = {\url{https://docs.nvidia.com/deeplearning/cudnn/api/index.html##cudnnMultiHeadAttnForward}},
    note = {Accessed on: 2023-11-25}
}

@misc{cutlass,
    title = {NVIDIA CUTLASS},
    year = {2023},
    howpublished = {\url{https://github.com/NVIDIA/cutlass}},
    note = {Commit: b5d8a5d, Accessed on: 2023-11-25}
}

@misc{tensorrtllm,
    title = {NVIDIA TensorRT-LLM},
    year = {2023},
    howpublished = {\url{https://github.com/NVIDIA/TensorRT-LLM}},
    note = {Commit: 6837c81, Accessed on: 2023-11-25}
}

@misc{openllm,
    title = {OpenLLM},
    year = {2023},
    howpublished = {\url{https://github.com/bentoml/OpenLLM}},
    note = {Commit: b4ea4b3, Accessed on: 2023-11-25}
}

@misc{rayllm,
    title = {RayLLM},
    year = {2023},
    howpublished = {\url{https://github.com/ray-project/ray-llm}},
    note = {Commit: fa3a766, Accessed on: 2023-11-25}
}

@misc{samba,
    title = {Sambanova},
    year = {2023},
    howpublished = {\url{https://sambanova.ai/press/sambanova-unveils-new-chip-the-sn40l/}}
}

@misc{vllm,
    title = {vLLM},
    year = {2023},
    howpublished = {\url{https://github.com/vllm-project/vllm}},
    note = {Commit: 7c60044, Accessed on: 2023-11-25}
}

@misc{xinference,
    title = {Xorbits Inference (Xinference)},
    year = {2023},
    howpublished = {\url{https://github.com/xorbitsai/inference}},
    note = {Commit: 22732d8, Accessed on: 2023-11-25}
}

@article{agrawal2023sarathi,
    author = {Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav S Gulavani and Ramachandran Ramjee},
    title = {SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills},
    journal = {arXiv preprint arXiv:2308.16369},
    year = {2023}
}

@article{ainslie2023gqa,
    author = {Joshua Ainslie and James Lee-Thorp and Michiel de Jong and Yury Zemlyanskiy and Federico Lebrn and Sumit Sanghai},
    title = {GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
    journal = {arXiv preprint arXiv:2305.13245},
    year = {2023}
}

@inproceedings{ali2020batch,
    author = {Ahsan Ali and Riccardo Pinciroli and Feng Yan and Evgenia Smirni},
    title = {Batch: machine learning inference serving on serverless platforms with adaptive batching},
    booktitle = {SC20: International Conference for High Performance Computing, Networking, Storage and Analysis},
    pages = {1--15},
    year = {2020},
    publisher = {IEEE}
}

@article{alizadeh2023llm,
    author = {Keivan Alizadeh and Iman Mirzadeh and Dmitry Belenko and Karen Khatamifard and Minsik Cho and Carlo C Del Mundo and Mohammad Rastegari and Mehrdad Farajtabar},
    title = {LLM in a flash: Efficient Large Language Model Inference with Limited Memory},
    journal = {arXiv preprint arXiv:2312.11514},
    year = {2023}
}

@article{aminabadi2022deepspeed,
    author = {Reza Yazdani Aminabadi and Samyam Rajbhandari and Minjia Zhang and Ammar Ahmad Awan and Cheng Li and Du Li and Elton Zheng and Jeff Rasley and Shaden Smith and Olatunji Ruwase and others},
    title = {Deepspeed inference: Enabling efficient inference of transformer models at unprecedented scale},
    journal = {arXiv preprint arXiv:2207.00032},
    year = {2022}
}

@article{anagnostidis2023dynamic,
    author = {Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hoffmann},
    title = {Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},
    journal = {arXiv preprint arXiv:2305.15805},
    year = {2023}
}

@article{bae2023fast,
    author = {Sangmin Bae and Jongwoo Ko and Hwanjun Song and Se-Young Yun},
    title = {Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding},
    journal = {arXiv preprint arXiv:2310.05424},
    year = {2023}
}

@inproceedings{bai2023longbench,
    author = {Yushi Bai and Xin Lv and Jiajie Zhang and Hongchang Lyu and Jiankai Tang and Zhidian Huang and Zhengxiao Du and Xiao Liu and Aohan Zeng and Lei Hou and others},
    title = {LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding},
    booktitle = {arXiv preprint arXiv:2308.14508},
    year = {2023}
}

@inproceedings{bai2020pipeswitch,
    author = {Zhihao Bai and Zhen Zhang and Yibo Zhu and Xin Jin},
    title = {PipeSwitch: Fast pipelined context switching for deep learning applications},
    booktitle = {14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
    pages = {499--514},
    year = {2020}
}

@article{belcak2023exponentially,
    author = {Peter Belcak and Roger Wattenhofer},
    title = {Exponentially Faster Language Modelling},
    journal = {arXiv preprint arXiv:2311.10770},
    year = {2023}
}

@article{beltagy2020longformer,
    author = {Iz Beltagy and Matthew E Peters and Arman Cohan},
    title = {Longformer: The long-document transformer},
    journal = {arXiv preprint arXiv:2004.05150},
    year = {2020}
}

@article{ben2019demystifying,
    author = {Tal Ben-Nun and Torsten Hoefler},
    title = {Demystifying parallel and distributed deep learning: An in-depth concurrency analysis},
    journal = {ACM Computing Surveys (CSUR)},
    volume = {52},
    number = {4},
    pages = {1--43},
    year = {2019}
}

@inproceedings{borgeaud2022improving,
    author = {Sebastian Borgeaud and Arthur Mensch and Jordan Hoffmann and Trevor Cai and Eliza Rutherford and Katie Millican and George Bm Van Den Driessche and Jean-Baptiste Lespiau and Bogdan Damoc and Aidan Clark and others},
    title = {Improving language models by retrieving from trillions of tokens},
    booktitle = {International conference on machine learning},
    pages = {2206--2240},
    year = {2022},
    publisher = {PMLR}
}

@article{borzunov2022petals,
    author = {Alexander Borzunov and Dmitry Baranchuk and Tim Dettmers and Max Ryabinin and Younes Belkada and Artem Chumachenko and Pavel Samygin and Colin Raffel},
    title = {Petals: Collaborative inference and fine-tuning of large models},
    journal = {arXiv preprint arXiv:2209.01188},
    year = {2022}
}

@article{borzunov2023distributed,
    author = {Alexander Borzunov and Max Ryabinin and Artem Chumachenko and Dmitry Baranchuk and Tim Dettmers and Younes Belkada and Pavel Samygin and Colin Raffel},
    title = {Distributed Inference and Fine-tuning of Large Language Models Over The Internet},
    journal = {arXiv preprint arXiv:2312.08361},
    year = {2023}
}

@article{bozic2023rethinking,
    author = {Vukasin Bozic and Danilo Dordevic and Daniele Coppola and Joseph Thommes},
    title = {Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers},
    journal = {arXiv preprint arXiv:2311.10642},
    year = {2023}
}

@article{burton1985speculative,
    author = {F Warren Burton},
    title = {Speculative computation, parallelism, and functional programming},
    journal = {IEEE Trans. Comput.},
    volume = {100},
    number = {12},
    pages = {1190--1193},
    year = {1985}
}

@inproceedings{cai2023medusa,
    author = {Tianle Cai and Yuhong Li and Zhengyang Geng and Hongwu Peng and Tri Dao},
    title = {Medusa: Simple framework for accelerating llm generation with multiple decoding heads},
    booktitle = {arXiv preprint arXiv:2304.07410},
    year = {2023}
}

@article{chand2023dsformer,
    author = {Rahul Chand and Yashoteja Prabhu and Pratyush Kumar},
    title = {DSFormer: Effective Compression of Text-Transformers by Dense-Sparse Weight Factorization},
    journal = {arXiv preprint arXiv:2312.13211},
    year = {2023}
}

@article{chen2022program,
    author = {Wenhu Chen and Xueguang Ma and Xinyi Wang and William W Cohen},
    title = {Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks},
    journal = {arXiv preprint arXiv:2211.12588},
    year = {2022}
}

@article{chen2023accelerating,
    author = {Charlie Chen and Sebastian Borgeaud and Geoffrey Irving and Jean-Baptiste Lespiau and Laurent Sifre and John Jumper},
    title = {Accelerating large language model decoding with speculative sampling},
    journal = {arXiv preprint arXiv:2302.01318},
    year = {2023}
}

@article{chen2023punica,
    author = {Lequn Chen and Zihao Ye and Yongji Wu and Danyang Zhuo and Luis Ceze and Arvind Krishnamurthy},
    title = {Punica: Multi-Tenant LoRA Serving},
    journal = {arXiv preprint arXiv:2310.18547},
    year = {2023}
}

@article{chen2023frugalgpt,
    author = {Lingjiao Chen and Matei Zaharia and James Zou},
    title = {FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance},
    journal = {arXiv preprint arXiv:2305.05176},
    year = {2023}
}

@inproceedings{chen2021evaluating,
    author = {Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and others},
    title = {Evaluating large language models trained on code},
    booktitle = {arXiv preprint arXiv:2107.03374},
    year = {2021}
}

@inproceedings{chen2021re,
    author = {Shiyang Chen and Shaoyi Huang and Santosh Pandey and Bingbing Li and Guang R Gao and Long Zheng and Caiwen Ding and Hang Liu},
    title = {Et: re-thinking self-attention for transformer models on gpus},
    booktitle = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    pages = {1--18},
    year = {2021}
}

@article{chen2023extending,
    author = {Shouyuan Chen and Sherman Wong and Liangjian Chen and Yuandong Tian},
    title = {Extending context window of large language models via positional interpolation},
    journal = {arXiv preprint arXiv:2306.15595},
    year = {2023}
}

@inproceedings{choi2022accelerating,
    author = {Jaewan Choi and Hailong Li and Byeongho Kim and Seunghwan Hwang and Jung Ho Ahn},
    title = {Accelerating transformer networks through recomposing softmax layers},
    booktitle = {2022 IEEE International Symposium on Workload Characterization (IISWC)},
    pages = {92--103},
    year = {2022}
}

@article{chowdhery2022palm,
    author = {Aakanksha Chowdhery and Sharan Narang and Jacob Devlin and Maarten Bosma and Gaurav Mishra and Adam Roberts and Paul Barham and Hyung Won Chung and Charles Sutton and Sebastian Gehrmann and others},
    title = {Palm: Scaling language modeling with pathways},
    journal = {arXiv preprint arXiv:2204.02311},
    year = {2022}
}

@inproceedings{correia2019adaptively,
    author = {Gon{\c{c}}alo M Correia and Vlad Niculae and Andr{\'e} FT Martins},
    title = {Adaptively Sparse Transformers},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    pages = {2174--2184},
    year = {2019}
}

@article{csordas2023switchhead,
    author = {R{\'o}bert Csord{\'a}s and Piotr Pi{\k{e}}kos and Kazuki Irie},
    title = {SwitchHead: Accelerating Transformers with Mixture-of-Experts Attention},
    journal = {arXiv preprint arXiv:2312.07987},
    year = {2023}
}

@article{dalvi2023llmebench,
    author = {Fahim Dalvi and Maram Hasanain and Sabri Boughorbel and Basel Mousi and Samir Abdaljalil and Nizi Nazar and Ahmed Abdelali and Shammur Absar Chowdhury and Hamdy Mubarak and Ahmed Ali and others},
    title = {LLMeBench: A Flexible Framework for Accelerating LLMs Benchmarking},
    journal = {arXiv preprint arXiv:2308.04945},
    year = {2023}
}

@misc{bestpractices,
    author = {Databricks},
    title = {LLM Inference Performance Engineering: Best Practices},
    year = {2023},
    howpublished = {\url{https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices}},
    note = {Accessed on: 2023-11-25}
}

@inproceedings{dauphin2017language,
    author = {Yann N Dauphin and Angela Fan and Michael Auli and David Grangier},
    title = {Language modeling with gated convolutional networks},
    booktitle = {International conference on machine learning},
    pages = {933--941},
    year = {2017},
    publisher = {PMLR}
}

@article{del2023skipdecode,
    author = {Luciano Del Corro and Allie Del Giorno and Sahaj Agarwal and Bin Yu and Ahmed Awadallah and Subhabrata Mukherjee},
    title = {SkipDecode: Autoregressive Skip Decoding with Batching and Caching for Efficient LLM Inference},
    journal = {arXiv preprint arXiv:2307.02628},
    year = {2023}
}

@article{dettmers2022llmint8,
    author = {Tim Dettmers and Mike Lewis and Younes Belkada and Luke Zettlemoyer},
    title = {LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale},
    journal = {arXiv preprint arXiv:2208.07339},
    year = {2022}
}

@article{dettmers2023qlora,
    author = {Tim Dettmers and Artidoro Pagnoni and Ari Holtzman and Luke Zettlemoyer},
    title = {Qlora: Efficient finetuning of quantized llms},
    journal = {arXiv preprint arXiv:2305.14314},
    year = {2023}
}

@article{dettmers2023spqr,
    author = {Tim Dettmers and Ruslan Svirschevski and Vage Egiazarian and Denis Kuznedelev and Elias Frantar and Saleh Ashkboos and Alexander Borzunov and Torsten Hoefler and Dan Alistarh},
    title = {SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight Compression},
    journal = {arXiv preprint arXiv:2306.03078},
    year = {2023}
}

@article{dettmers2022case,
    author = {Tim Dettmers and Luke Zettlemoyer},
    title = {The case for 4-bit precision: k-bit Inference Scaling Laws},
    journal = {arXiv preprint arXiv:2212.09720},
    year = {2022}
}

@article{dey2023cerebras,
    author = {Nolan Dey and Gurpreet Gosal and Hemant Khachane and William Marshall and Ribhu Pathria and Marvin Tom and Joel Hestness and others},
    title = {Cerebras-GPT: Open compute-optimal language models trained on the Cerebras wafer-scale cluster},
    journal = {arXiv preprint arXiv:2304.03208},
    year = {2023}
}

@article{ding2023longnet,
    author = {Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Furu Wei},
    title = {Longnet: Scaling transformers to 1,000,000,000 tokens},
    journal = {arXiv preprint arXiv:2307.02486},
    year = {2023}
}

@inproceedings{dong2023towards,
    author = {Xin Luna Dong and Seungwhan Moon and Yifan Ethan Xu and Kshitiz Malik and Zhou Yu},
    title = {Towards Next-Generation Intelligent Assistants Leveraging LLM Techniques},
    booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    pages = {5792--5793},
    year = {2023}
}

@article{du2023improving,
    author = {Jiangsu Du and Jiazhi Jiang and Jiang Zheng and Hongbin Zhang and Dan Huang and Yutong Lu},
    title = {Improving Computation and Memory Efficiency for Real-world Transformer Inference on GPUs},
    journal = {ACM Transactions on Architecture and Code Optimization},
    volume = {20},
    number = {4},
    pages = {1--22},
    year = {2023}
}

@inproceedings{du2022glam,
    author = {Nan Du and Yanping Huang and Andrew M Dai and Simon Tong and Dmitry Lepikhin and Yuanzhong Xu and Maxim Krikun and Yanqi Zhou and Adams Wei Yu and Orhan Firat and others},
    title = {Glam: Efficient scaling of language models with mixture-of-experts},
    booktitle = {International Conference on Machine Learning},
    pages = {5547--5569},
    year = {2022}
}

@article{emani2023comprehensive,
    author = {Murali Emani and Sam Foreman and Varuni Sastry and Zhen Xie and Siddhisanket Raskar and William Arnold and Rajeev Thakur and Venkatram Vishwanath and Michael E Papka},
    title = {A Comprehensive Performance Study of Large Language Models on Novel AI Accelerators},
    journal = {arXiv preprint arXiv:2310.04607},
    year = {2023}
}

@article{faiz2023llmcarbon,
    author = {Ahmad Faiz and Sotaro Kaneda and Ruhan Wang and Rita Osi and Parteek Sharma and Fan Chen and Lei Jiang},
    title = {LLMCarbon: Modeling the end-to-end Carbon Footprint of Large Language Models},
    journal = {arXiv preprint arXiv:2309.14393},
    year = {2023}
}

@inproceedings{fan2019reducing,
    author = {Angela Fan and Edouard Grave and Armand Joulin},
    title = {Reducing Transformer Depth on Demand with Structured Dropout},
    booktitle = {International Conference on Learning Representations},
    year = {2019}
}

@inproceedings{fan2018hierarchical,
    author = {Angela Fan and Mike Lewis and Yann Dauphin},
    title = {Hierarchical Neural Story Generation},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages = {889--898},
    year = {2018}
}

@inproceedings{fang2021turbotransformers,
    author = {Jiarui Fang and Yang Yu and Chengduo Zhao and Jie Zhou},
    title = {Turbotransformers: an efficient gpu serving system for transformer models},
    booktitle = {Proceedings of the 26th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
    pages = {389--402},
    year = {2021}
}

@article{fedus2022switch,
    author = {William Fedus and Barret Zoph and Noam Shazeer},
    title = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
    journal = {The Journal of Machine Learning Research},
    volume = {23},
    number = {1},
    pages = {5232--5270},
    year = {2022}
}

@article{fegade2022cora,
    author = {Pratik Fegade and Tianqi Chen and Phillip Gibbons and Todd Mowry},
    title = {The CoRa tensor compiler: Compilation for ragged tensors with minimal padding},
    journal = {Proceedings of Machine Learning and Systems},
    volume = {4},
    pages = {721--747},
    year = {2022}
}

@article{fei2023extending,
    author = {Weizhi Fei and Xueyan Niu and Pingyi Zhou and Lu Hou and Bo Bai and Lei Deng and Wei Han},
    title = {Extending Context Window of Large Language Models via Semantic Compression},
    journal = {arXiv preprint arXiv:2312.09571},
    year = {2023}
}

@inproceedings{feng2023tensorir,
    author = {Siyuan Feng and Bohan Hou and Hongyi Jin and Wuwei Lin and Junru Shao and Ruihang Lai and Zihao Ye and Lianmin Zheng and Cody Hao Yu and Yong Yu and others},
    title = {Tensorir: An abstraction for automatic tensorized program optimization},
    booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
    pages = {804--817},
    year = {2023}
}

@article{frantar2023sparsegpt,
    author = {Elias Frantar and Dan Alistarh},
    title = {SparseGPT: Massive Language Models Can Be Accurately Pruned in One-Shot},
    journal = {arXiv preprint arXiv:2311.00737},
    year = {2023}
}

@article{frantar2022gptq,
    author = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
    title = {Gptq: Accurate post-training quantization for generative pre-trained transformers},
    journal = {arXiv preprint arXiv:2210.17323},
    year = {2022}
}

@inproceedings{frantar2022optq,
    author = {Elias Frantar and Saleh Ashkboos and Torsten Hoefler and Dan Alistarh},
    title = {OPTQ: Accurate quantization for generative pre-trained transformers},
    booktitle = {The Eleventh International Conference on Learning Representations},
    year = {2022}
}

@article{frostig2018compiling,
    author = {Roy Frostig and Matthew James Johnson and Chris Leary},
    title = {Compiling machine learning programs via high-level tracing},
    journal = {Systems for Machine Learning},
    volume = {4},
    number = {9},
    year = {2018}
}

@inproceedings{fu2022hungry,
    author = {Daniel Y Fu and Tri Dao and Khaled Kamal Saab and Armin W Thomas and Atri Rudra and Christopher Re},
    title = {Hungry Hungry Hippos: Towards Language Modeling with State Space Models},
    booktitle = {The Eleventh International Conference on Learning Representations},
    year = {2022}
}

@inproceedings{fu2023lookahead,
    author = {Yichao Fu and Peter Bailis and Ion Stoica and Hao Zhang},
    title = {Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding},
    booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    year = {2023}
}

@inproceedings{gale2023megablocks,
    author = {Trevor Gale and Deepak Narayanan and Cliff Young and Matei Zaharia},
    title = {MegaBlocks: Efficient Sparse Training with Mixture-of-Experts},
    booktitle = {Proceedings of Machine Learning and Systems},
    volume = {5},
    year = {2023}
}

@article{ge2023model,
    author = {Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
    title = {Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs},
    journal = {Workshop on Advancing Neural Network Training: Computational Efficiency, Scalability, and Resource Optimization (WANT@ NeurIPS 2023)},
    year = {2023}
}

@article{ge2023context,
    author = {Tao Ge and Jing Hu and Xun Wang and Si-Qing Chen and Furu Wei},
    title = {In-context autoencoder for context compression in a large language model},
    journal = {arXiv preprint arXiv:2307.06945},
    year = {2023}
}

@article{ge2022lossless,
    author = {Tao Ge and Heming Xia and Xin Sun and Si-Qing Chen and Furu Wei},
    title = {Lossless acceleration for Seq2seq generation with aggressive decoding},
    journal = {arXiv preprint arXiv:2205.10350},
    year = {2022}
}

@inproceedings{ghazvininejad2019mask,
    author = {Marjan Ghazvininejad and Omer Levy and Yinhan Liu and Luke Zettlemoyer},
    title = {Mask-Predict: Parallel Decoding of Conditional Masked Language Models},
    booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    pages = {6112--6121},
    year = {2019}
}

@article{ghazvininejad2020semi,
    author = {Marjan Ghazvininejad and Omer Levy and Luke Zettlemoyer},
    title = {Semi-autoregressive training improves mask-predict decoding},
    journal = {arXiv preprint arXiv:2001.08785},
    year = {2020}
}

@inproceedings{gholami2022survey,
    author = {Amir Gholami and Sehoon Kim and Zhen Dong and Zhewei Yao and Michael W Mahoney and Kurt Keutzer},
    title = {A survey of quantization methods for efficient neural network inference},
    booktitle = {Low-Power Computer Vision},
    publisher = {Chapman and Hall/CRC},
    pages = {291--326},
    year = {2022}
}

@article{gim2023prompt,
    author = {In Gim and Guojun Chen and Seung-seob Lee and Nikhil Sarda and Anurag Khandelwal and Lin Zhong},
    title = {Prompt Cache: Modular Attention Reuse for Low-Latency Inference},
    journal = {arXiv preprint arXiv:2311.04934},
    year = {2023}
}

@inproceedings{goyal2020power,
    author = {Saurabh Goyal and Anamitra Roy Choudhury and Saurabh Raje and Venkatesan Chakaravarthy and Yogish Sabharwal and Ashish Verma},
    title = {PoWER-BERT: Accelerating BERT inference via progressive word-vector elimination},
    booktitle = {International Conference on Machine Learning},
    pages = {3690--3699},
    year = {2020}
}

@article{gu2023mamba,
    author = {Albert Gu and Tri Dao},
    title = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
    journal = {arXiv preprint arXiv:2312.00752},
    year = {2023}
}

@inproceedings{gu2021efficiently,
    author = {Albert Gu and Karan Goel and Christopher Re},
    title = {Efficiently Modeling Long Sequences with Structured State Spaces},
    booktitle = {International Conference on Learning Representations},
    year = {2021}
}

@inproceedings{gu2018non,
    author = {J Gu and J Bradbury and C Xiong and VOK Li and R Socher},
    title = {Non-autoregressive neural machine translation},
    booktitle = {International Conference on Learning Representations (ICLR)},
    year = {2018}
}

@inproceedings{gu2021fully,
    author = {Jiatao Gu and Xiang Kong},
    title = {Fully Non-autoregressive Neural Machine Translation: Tricks of the Trade},
    booktitle = {Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021},
    pages = {120--133},
    year = {2021}
}

@article{gu2023knowledge,
    author = {Yuxian Gu and Li Dong and Furu Wei and Minlie Huang},
    title = {Knowledge Distillation of Large Language Models},
    journal = {arXiv preprint arXiv:2306.08543},
    year = {2023}
}

@inproceedings{gunasekaran2022cocktail,
    author = {Jashwant Raj Gunasekaran and Cyan Subhra Mishra and Prashanth Thinakaran and Bikash Sharma and Mahmut Taylan Kandemir and Chita R Das},
    title = {Cocktail: A multidimensional optimization for model serving in cloud},
    booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
    pages = {1041--1057},
    year = {2022}
}

@article{guo2019non,
    author = {Junliang Guo and Xu Tan and Di He and Tao Qin and Linli Xu and Tie-Yan Liu},
    title = {Non-autoregressive neural machine translation with enhanced decoder input},
    journal = {Proceedings of the AAAI conference on artificial intelligence},
    volume = {33},
    pages = {3723--3730},
    year = {2019}
}

@inproceedings{guo2023sti,
    author = {Liwei Guo and Wonkyo Choe and Felix Xiaozhu Lin},
    title = {STI: Turbocharge NLP Inference at the Edge via Elastic Pipelining},
    booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
    pages = {791--803},
    year = {2023}
}

@inproceedings{guo2019star,
    author = {Qipeng Guo and Xipeng Qiu and Pengfei Liu and Yunfan Shao and Xiangyang Xue and Zheng Zhang},
    title = {Star-Transformer},
    booktitle = {Proceedings of NAACL-HLT},
    pages = {1315--1325},
    year = {2019}
}

@article{gupta2020gmat,
    author = {Ankit Gupta and Jonathan Berant},
    title = {Gmat: Global memory augmentation for transformers},
    journal = {arXiv preprint arXiv:2006.03274},
    year = {2020}
}

@inproceedings{gupta2021memory,
    author = {Ankit Gupta and Guy Dar and Shaya Goodman and David Ciprut and Jonathan Berant},
    title = {Memory-efficient Transformers via Top-k Attention},
    booktitle = {Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
    pages = {39--52},
    year = {2021}
}

@article{gupta2022compression,
    author = {Manish Gupta and Puneet Agrawal},
    title = {Compression of deep learning models for text: A survey},
    journal = {ACM Transactions on Knowledge Discovery from Data (TKDD)},
    volume = {16},
    number = {4},
    pages = {1--55},
    year = {2022}
}

@inproceedings{han2022microsecond,
    author = {Mingcong Han and Hanze Zhang and Rong Chen and Haibo Chen},
    title = {Microsecond-scale preemption for concurrent DNN inferences},
    booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
    pages = {539--558},
    year = {2022}
}

@article{he2023simplifying,
    author = {Bobby He and Thomas Hofmann},
    title = {Simplifying Transformer Blocks},
    journal = {arXiv preprint arXiv:2311.01906},
    year = {2023}
}

@inproceedings{he2022fastermoe,
    author = {Jiaao He and Jidong Zhai and Tiago Antunes and Haojie Wang and Fuwen Luo and Shangfeng Shi and Qin Li},
    title = {FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models},
    booktitle = {Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
    pages = {120--134},
    year = {2022}
}

@inproceedings{he2021magic,
    author = {Xuanli He and Iman Keivanloo and Yi Xu and Xiang He and Belinda Zeng and Santosh Rajagopalan and Trishul Chilimbi},
    title = {Magic pyramid: Accelerating inference with early exiting and token pruning},
    booktitle = {arXiv preprint arXiv:2111.00230},
    year = {2021}
}

@article{he2023rest,
    author = {Zhenyu He and Zexuan Zhong and Tianle Cai and Jason D Lee and Di He},
    title = {REST: Retrieval-Based Speculative Decoding},
    journal = {arXiv preprint arXiv:2311.08252},
    year = {2023}
}

@article{holtzman2019curious,
    author = {Ari Holtzman and Jan Buys and Li Du and Maxwell Forbes and Yejin Choi},
    title = {The curious case of neural text degeneration},
    journal = {arXiv preprint arXiv:1904.09751},
    year = {2019}
}

@article{hong2023flashdecoding++,
    author = {Ke Hong and Guohao Dai and Jiaming Xu and Qiuli Mao and Xiuhong Li and Jun Liu and Kangdi Chen and Hanyu Dong and Yu Wang},
    title = {FlashDecoding++: Faster Large Language Model Inference on GPUs},
    journal = {arXiv preprint arXiv:2311.01282},
    year = {2023}
}

@article{hooper2023speed,
    author = {Coleman Hooper and Sehoon Kim and Hiva Mohammadzadeh and Hasan Genc and Kurt Keutzer and Amir Gholami and Sophia Shao},
    title = {SPEED: Speculative Pipelined Execution for Efficient Decoding},
    journal = {arXiv preprint arXiv:2310.12072},
    year = {2023}
}

@inproceedings{huang2023towards,
    author = {Haiyang Huang and Newsha Ardalani and Anna Sun and Liu Ke and Hsien-Hsin S Lee and Anjali Sridhar and Shruti Bhosale and Carole-Jean Wu and Benjamin Lee},
    title = {Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference},
    booktitle = {arXiv preprint arXiv:2303.06182},
    year = {2023}
}

@inproceedings{hwang2023tutel,
    author = {Changho Hwang and Wei Cui and Yifan Xiong and Ziyue Yang and Ze Liu and Han Hu and Zilong Wang and Rafael Salas and Jithin Jose and Prabhat Ram and others},
    title = {Tutel: Adaptive mixture-of-experts at scale},
    booktitle = {Proceedings of Machine Learning and Systems},
    volume = {5},
    year = {2023}
}

@article{isaev2023calculon,
    author = {Mikhail Isaev and Nic Mcdonald and Larry Dennison and Richard Vuduc},
    title = {Calculon: a methodology and tool for high-level co-design of systems and large language models},
    journal = {Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
    pages = {1--14},
    year = {2023}
}

@inproceedings{isik2023gpt,
    author = {Berivan Isik and Hermann Kumbong and Wanyi Ning and Xiaozhe Yao and Sanmi Koyejo and Ce Zhang},
    title = {GPT-Zip: Deep Compression of Finetuned Large Language Models},
    booktitle = {Workshop on Efficient Systems for Foundation Models@ ICML2023},
    year = {2023}
}

@inproceedings{jaiswal2023compressing,
    author = {Ajay Jaiswal and Zhe Gan and Xianzhi Du and Bowen Zhang and Zhangyang Wang and Yinfei Yang},
    title = {Compressing LLMs: The Truth is Rarely Pure and Never Simple},
    booktitle = {arXiv preprint arXiv:2310.01382},
    year = {2023}
}

@inproceedings{jia2019taso,
    author = {Zhihao Jia and Oded Padon and James Thomas and Todd Warszawski and Matei Zaharia and Alex Aiken},
    title = {TASO: optimizing deep learning computation with automatic generation of graph substitutions},
    booktitle = {Proceedings of the 27th ACM Symposium on Operating Systems Principles},
    pages = {47--62},
    year = {2019}
}

@inproceedings{jia2019beyond,
    author = {Zhihao Jia and Matei Zaharia and Alex Aiken},
    title = {Beyond Data and Model Parallelism for Deep Neural Networks.},
    booktitle = {Proceedings of Machine Learning and Systems},
    volume = {1},
    pages = {1--13},
    year = {2019}
}

@article{jiang2023mistral,
    author = {Albert Q Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and others},
    title = {Mistral 7B},
    journal = {arXiv preprint arXiv:2310.06825},
    year = {2023}
}

@article{jiang2023llmlingua,
    author = {Huiqiang Jiang and Qianhui Wu and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
    title = {Llmlingua: Compressing prompts for accelerated inference of large language models},
    journal = {arXiv preprint arXiv:2310.05736},
    year = {2023}
}

@article{jiang2023longllmlingua,
    author = {Huiqiang Jiang and Qianhui Wu and Xufang Luo and Dongsheng Li and Chin-Yew Lin and Yuqing Yang and Lili Qiu},
    title = {LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression},
    journal = {arXiv preprint arXiv:2310.06839},
    year = {2023}
}

@article{jiang2023hexgen,
    author = {Youhe Jiang and Ran Yan and Xiaozhe Yao and Beidi Chen and Binhang Yuan},
    title = {HexGen: Generative Inference of Foundation Model over Heterogeneous Decentralized Environment},
    journal = {arXiv preprint arXiv:2311.11514},
    year = {2023}
}

@inproceedings{jiao2019tinybert,
    author = {Xiaoqi Jiao and Yichun Yin and Lifeng Shang and Xin Jiang and Xiao Chen and Linlin Li and Fang Wang and Qun Liu},
    title = {TinyBERT: Distilling BERT for Natural Language Understanding},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2020},
    pages = {4163--4174},
    year = {2020}
}

@article{jin2023s,
    author = {Yunho Jin and Chun-Feng Wu and David Brooks and Gu-Yeon Wei},
    title = {S$^3$: Increasing GPU Utilization during Generative Inference for Higher Throughput},
    journal = {arXiv preprint arXiv:2306.06000},
    year = {2023}
}

@article{jo2023promise,
    author = {A Jo},
    title = {The promise and peril of generative AI},
    journal = {Nature},
    volume = {614},
    number = {1},
    pages = {214--216},
    year = {2023}
}

@inproceedings{jouppi2023tpu,
    author = {Norm Jouppi and George Kurian and Sheng Li and Peter Ma and Rahul Nagarajan and Lifeng Nai and Nishant Patil and Suvinay Subramanian and Andy Swing and Brian Towles and others},
    title = {Tpu v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings},
    booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
    pages = {1--14},
    year = {2023}
}

@inproceedings{kasai2020deep,
    author = {Jungo Kasai and Nikolaos Pappas and Hao Peng and James Cross and Noah Smith},
    title = {Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation},
    booktitle = {International Conference on Learning Representations},
    year = {2020}
}

@inproceedings{katel2022mlir,
    author = {Navdeep Katel and Vivek Khandelwal and Uday Bondhugula},
    title = {MLIR-based code generation for GPU tensor cores},
    booktitle = {Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction},
    pages = {117--128},
    year = {2022}
}

@inproceedings{katharopoulos2020transformers,
    author = {Angelos Katharopoulos and Apoorv Vyas and Nikolaos Pappas and Fran{\c{c}}ois Fleuret},
    title = {Transformers are rnns: Fast autoregressive transformers with linear attention},
    booktitle = {International conference on machine learning},
    pages = {5156--5165},
    year = {2020}
}

@article{keskar2019ctrl,
    author = {Nitish Shirish Keskar and Bryan McCann and Lav R Varshney and Caiming Xiong and Richard Socher},
    title = {Ctrl: A conditional transformer language model for controllable generation},
    journal = {arXiv preprint arXiv:1909.05858},
    year = {2019}
}

@inproceedings{khandelwal2018sharp,
    author = {Urvashi Khandelwal and He He and Peng Qi and Dan Jurafsky},
    title = {Sharp Nearby, Fuzzy Far Away: How Neural Language Models Use Context},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages = {284--294},
    year = {2018}
}

@inproceedings{kim2023memory,
    author = {Jeonghoon Kim and Jung Hyun Lee and Sungdong Kim and Joonsuk Park and Kang Min Yoo and Se Jung Kwon and Dongsoo Lee},
    title = {Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization},
    booktitle = {arXiv preprint arXiv:2305.14152},
    year = {2023}
}

@inproceedings{kim2023squeezellm,
    author = {Sehoon Kim and Coleman Hooper and Amir Gholami and Zhen Dong and Xiuyu Li and Sheng Shen and Michael W Mahoney and Kurt Keutzer},
    title = {SqueezeLLM: Dense-and-Sparse Quantization},
    booktitle = {arXiv preprint arXiv:2306.07629},
    year = {2023}
}

@inproceedings{kim2023full,
    author = {Sehoon Kim and Coleman Hooper and Thanakul Wattanawong and Minwoo Kang and Ruohan Yan and Hasan Genc and Grace Dinh and Qijing Huang and Kurt Keutzer and Michael W Mahoney and others},
    title = {Full stack optimization of transformer inference: a survey},
    booktitle = {arXiv preprint arXiv:2302.14017},
    year = {2023}
}

@article{kim2023big,
    author = {Sehoon Kim and Karttikeya Mangalam and Jitendra Malik and Michael W Mahoney and Amir Gholami and Kurt Keutzer},
    title = {Big little transformer decoder},
    journal = {arXiv preprint arXiv:2302.07863},
    year = {2023}
}

@article{kitaev2019reformer,
    author = {Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
    title = {Reformer: The Efficient Transformer},
    journal = {International Conference on Learning Representations},
    year = {2019}
}

@inproceedings{kong2022accelerating,
    author = {Jun Kong and Jin Wang and Liang-Chih Yu and Xuejie Zhang},
    title = {Accelerating Inference for Pretrained Language Models by Unified Multi-Perspective Early Exiting},
    booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
    pages = {4677--4686},
    year = {2022}
}

@inproceedings{kudugunta2021beyond,
    author = {Sneha Kudugunta and Yanping Huang and Ankur Bapna and Maxim Krikun and Dmitry Lepikhin and Minh-Thang Luong and Orhan Firat},
    title = {Beyond Distillation: Task-level Mixture-of-Experts for Efficient Inference},
    booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
    pages = {3577--3599},
    year = {2021}
}

@inproceedings{kurtic2023ziplm,
    author = {Eldar Kurtic and Elias Frantar and Dan Alistarh},
    title = {Ziplm: Hardware-aware structured pruning of language models},
    booktitle = {arXiv preprint arXiv:2302.04089},
    year = {2023}
}

@inproceedings{kwon2023efficient,
    author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E Gonzalez and Hao Zhang and Ion Stoica},
    title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
    booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
    pages = {611--626},
    year = {2023}
}

@inproceedings{lai2023relax,
    author = {Ruihang Lai and Junru Shao and Siyuan Feng and Steven S Lyubomirsky and Bohan Hou and Wuwei Lin and Zihao Ye and Hongyi Jin and Yuchen Jin and Jiawei Liu and others},
    title = {Relax: Composable Abstractions for End-to-End Dynamic Machine Learning},
    booktitle = {arXiv preprint arXiv:2311.02103},
    year = {2023}
}

@inproceedings{lee2018deterministic,
    author = {Jason Lee and Elman Mansimov and Kyunghyun Cho},
    title = {Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement},
    booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
    year = {2018}
}

@misc{Lefaudeux2022,
    author = {Benjamin Lefaudeux and Francisco Massa and Diana Liskovich and Wenhan Xiong and Vittorio Caggiano and Sean Naren and Min Xu and Jieru Hu and Marta Tintore and Susan Zhang and Patrick Labatut and Daniel Haziza},
    title = {xFormers: A modular and hackable Transformer modelling library},
    howpublished = {\url{https://github.com/facebookresearch/xformers}},
    note = {Commit: fbf349a, Accessed on: 2023-11-25}
}

@inproceedings{lepikhin2020gshard,
    author = {Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
    title = {GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
    booktitle = {International Conference on Learning Representations},
    year = {2020}
}

@article{leviathan2023fast,
    author = {Yaniv Leviathan and Matan Kalman and Yossi Matias},
    title = {Fast inference from transformers via speculative decoding},
    journal = {International Conference on Machine Learning},
    pages = {19274--19286},
    year = {2023}
}

@misc{li2023accelerating,
    author = {Jiamin Li and Yimin Jiang and Yibo Zhu and Cong Wang and Hong Xu},
    title = {Accelerating Distributed MoE Training and Inference with Lina},
    howpublished = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
    note = {Pages 945--959}
}

@article{li2020cascadebert,
    author = {Lei Li and Yankai Lin and Deli Chen and Shuhuai Ren and Peng Li and Jie Zhou and Xu Sun},
    title = {Cascadebert: Accelerating inference of pre-trained language models via calibrated complete models cascade},
    journal = {arXiv preprint arXiv:2012.14682},
    year = {2020}
}

@article{li2023speed,
    author = {Qingyuan Li and Ran Meng and Yiduo Li and Bo Zhang and Liang Li and Yifan Lu and Xiangxiang Chu and Yerui Sun and Yuchen Xie},
    title = {A Speed Odyssey for Deployable Quantization of LLMs},
    journal = {arXiv preprint arXiv:2311.09550},
    year = {2023}
}

@article{li2023compressing,
    author = {Yucheng Li and Bo Dong and Frank Guerin and Chenghua Lin},
    title = {Compressing Context to Enhance Inference Efficiency of Large Language Models},
    journal = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages = {6342--6353},
    year = {2023}
}

@article{li2021efficient,
    author = {Yanyang Li and Ye Lin and Tong Xiao and Jingbo Zhu},
    title = {An efficient transformer decoder with compressed sub-layers},
    journal = {{AAA}I Conference on Artificial Intelligence},
    volume = {35},
    pages = {13315--13323},
    year = {2021}
}

@article{li2023losparse,
    author = {Yixiao Li and Yifan Yu and Qingru Zhang and Chen Liang and Pengcheng He and Weizhu Chen and Tuo Zhao},
    title = {LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation},
    journal = {arXiv preprint arXiv:2306.11222},
    year = {2023}
}

@article{li2023alpaserve,
    author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph E Gonzalez and others},
    title = {AlpaServe: Statistical Multiplexing with Model Parallelism for Deep Learning Serving},
    journal = {Proceedings of the 17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
    pages = {663--679},
    year = {2023}
}

@inproceedings{liao2021global,
    author = {Kaiyuan Liao and Yi Zhang and Xuancheng Ren and Qi Su and Xu Sun and Bin He},
    title = {A global past-future early exit method for accelerating inference of pre-trained language models},
    booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages = {2013--2023},
    year = {2021}
}

@article{lin2023awq,
    author = {Ji Lin and Jiaming Tang and Haotian Tang and Shang Yang and Xingyu Dang and Song Han},
    title = {AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration},
    journal = {arXiv preprint arXiv:2306.00978},
    year = {2023}
}

@article{liu2023ring,
    author = {Hao Liu and Matei Zaharia and Pieter Abbeel},
    title = {Ring Attention with Blockwise Transformers for Near-Infinite Context},
    journal = {arXiv preprint arXiv:2310.01889},
    year = {2023}
}

@article{liu2023lost,
    author = {Nelson F Liu and Kevin Lin and John Hewitt and Ashwin Paranjape and Michele Bevilacqua and Fabio Petroni and Percy Liang},
    title = {Lost in the middle: How language models use long contexts},
    journal = {arXiv preprint arXiv:2307.03172},
    year = {2023}
}

@inproceedings{liu2020fastbert,
    author = {Weijie Liu and Peng Zhou and Zhiruo Wang and Zhe Zhao and Haotang Deng and Qi Ju},
    title = {FastBERT: a Self-distilling BERT with Adaptive Inference Time},
    booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    pages = {6035--6044},
    year = {2020}
}

@inproceedings{liu2023online,
    author = {Xiaoxuan Liu and Lanxiang Hu and Peter Bailis and Ion Stoica and Zhijie Deng and Alvin Cheung and Hao Zhang},
    title = {Online Speculative Decoding},
    booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
    pages = {792--804},
    year = {2023}
}

@inproceedings{liu2023cachegen,
    author = {Yuhan Liu and Hanchen Li and Kuntai Du and Jiayi Yao and Yihua Cheng and Yuyang Huang and Shan Lu and Michael Maire and Henry Hoffmann and Ari Holtzman and others},
    title = {CacheGen: Fast Context Loading for Language Model Applications},
    booktitle = {arXiv preprint arXiv:2310.07240},
    year = {2023}
}

@article{liu2023scissorhands,
    author = {Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava},
    title = {Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time},
    journal = {arXiv preprint arXiv:2305.17118},
    year = {2023}
}

@article{liu2023llm,
    author = {Zechun Liu and Barlas Oguz and Changsheng Zhao and Ernie Chang and Pierre Stock and Yashar Mehdad and Yangyang Shi and Raghuraman Krishnamoorthi and Vikas Chandra},
    title = {LLM-QAT: Data-Free Quantization Aware Training for Large Language Models},
    journal = {arXiv preprint arXiv:2305.17888},
    year = {2023}
}

@inproceedings{liu2023deja,
    author = {Zichang Liu and Jue Wang and Tri Dao and Tianyi Zhou and Binhang Yuan and Zhao Song and Anshumali Shrivastava and Ce Zhang and Yuandong Tian and Christopher Re and others},
    title = {Deja vu: Contextual sparsity for efficient llms at inference time},
    booktitle = {International Conference on Machine Learning},
    pages = {22137--22176},
    year = {2023}
}

@inproceedings{lu2023bumblebee,
    author = {Wen-jie Lu and Zhicong Huang and Zhen Gu and Jingyu Li and Jian Liu and Kui Ren and Cheng Hong and Tao Wei and WenGuang Chen},
    title = {BumbleBee: Secure Two-party Inference Framework for Large Transformers},
    booktitle = {Cryptology ePrint Archive},
    year = {2023}
}

@article{ma2023llm,
    author = {Xinyin Ma and Gongfan Fang and Xinchao Wang},
    title = {LLM-Pruner: On the Structural Pruning of Large Language Models},
    journal = {arXiv preprint arXiv:2305.11627},
    year = {2023}
}

@article{mayer2020scalable,
    author = {Ruben Mayer and Hans-Arno Jacobsen},
    title = {Scalable deep learning on distributed infrastructures: Challenges, techniques, and tools},
    journal = {ACM Computing Surveys (CSUR)},
    volume = {53},
    number = {1},
    pages = {1--37},
    year = {2020}
}

@inproceedings{mehta2022long,
    author = {Harsh Mehta and Ankit Gupta and Ashok Cutkosky and Behnam Neyshabur},
    title = {Long Range Language Modeling via Gated State Spaces},
    booktitle = {The Eleventh International Conference on Learning Representations},
    year = {2022}
}

@article{miao2023specinfer,
    author = {Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Rae Ying Yee Wong and Zhuoming Chen and Daiyaan Arfeen and Reyna Abhyankar and Zhihao Jia},
    title = {SpecInfer: Accelerating Generative LLM Serving with Speculative Inference and Token Tree Verification},
    journal = {arXiv preprint arXiv:2305.09781},
    year = {2023}
}

@inproceedings{miao2024spotserve,
    author = {Xupeng Miao and Chunan Shi and Jiangfei Duan and Xiaoli Xi and Dahua Lin and Bin Cui},
    title = {SpotServe: Serving Generative Large Language Models on Preemptible Instances},
    booktitle = {Proceedings of ASPLOS Conference},
    year = {2024}
}

@article{miao2023galvatron,
    author = {Xupeng Miao and Yujie Wang and Youhe Jiang and Chunan Shi and Xiaonan Nie and Hailin Zhang and Bin Cui},
    title = {Galvatron: Efficient Transformer Training over Multiple GPUs Using Automatic Parallelism},
    journal = {Proceedings of the ACM on Management of Data},
    volume = {1},
    number = {1},
    pages = {1--19},
    year = {2023}
}

@article{michel2019sixteen,
    author = {Paul Michel and Omer Levy and Graham Neubig},
    title = {Are sixteen heads really better than one?},
    journal = {Advances in neural information processing systems},
    volume = {32},
    year = {2019}
}

@article{milakov2018online,
    author = {Maxim Milakov and Natalia Gimelshein},
    title = {Online normalizer calculation for softmax},
    journal = {arXiv preprint arXiv:1805.02867},
    year = {2018}
}

@article{mishra2021accelerating,
    author = {Asit Mishra and Jorge Albericio Latorre and Jeff Pool and Darko Stosic and Dusan Stosic and Ganesh Venkatesh and Chong Yu and Paulius Micikevicius},
    title = {Accelerating sparse deep neural networks},
    journal = {arXiv preprint arXiv:2104.08378},
    year = {2021}
}

@inproceedings{modarressi2022adapler,
    author = {Ali Modarressi and Hosein Mohebbi and Mohammad Taher Pilehvar},
    title = {Adapler: Speeding up inference by adaptive length reduction},
    booktitle = {arXiv preprint arXiv:2203.08991},
    year = {2022}
}

@article{mohtashami2023landmark,
    author = {Amirkeivan Mohtashami and Martin Jaggi},
    title = {Landmark Attention: Random-Access Infinite Context Length for Transformers},
    journal = {arXiv preprint arXiv:2305.16300},
    year = {2023}
}

@article{monea2023pass,
    author = {Giovanni Monea and Armand Joulin and Edouard Grave},
    title = {PaSS: Parallel Speculative Sampling},
    journal = {arXiv preprint arXiv:2311.13581},
    year = {2023}
}

@article{mu2023learning,
    author = {Jesse Mu and Xiang Lisa Li and Noah Goodman},
    title = {Learning to compress prompts with gist tokens},
    journal = {arXiv preprint arXiv:2304.08467},
    year = {2023}
}

@article{muhlgay2023generating,
    author = {Dor Muhlgay and Ori Ram and Inbal Magar and Yoav Levine and Nir Ratner and Yonatan Belinkov and Omri Abend and Kevin Leyton-Brown and Amnon Shashua and Yoav Shoham},
    title = {Generating benchmarks for factuality evaluation of language models},
    journal = {arXiv preprint arXiv:2307.06908},
    year = {2023}
}

@article{nagrecha2023saturn,
    author = {Kabir Nagrecha and Arun Kumar},
    title = {Saturn: An Optimized Data System for Large Model Deep Learning Workloads},
    journal = {arXiv preprint arXiv:2309.01226},
    year = {2023}
}

@inproceedings{narayanan2021memory,
    author = {Deepak Narayanan and Amar Phanishayee and Kaiyu Shi and Xie Chen and Omer Yilmaz and others},
    title = {Memory-efficient pipeline-parallel dnn training},
    booktitle = {International Conference on Machine Learning},
    pages = {7937--7947},
    year = {2021}
}

@article{narayanan2023cheaply,
    author = {Deepak Narayanan and Keshav Santhanam and Peter Henderson and Rishi Bommasani and Tony Lee and Percy Liang},
    title = {Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models},
    journal = {Thirty-seventh Conference on Neural Information Processing Systems},
    year = {2023}
}

@article{ng2023paella,
    author = {Kelvin KW Ng and Henri Maxime Demoulin and Vincent Liu},
    title = {Paella: Low-latency Model Serving with Software-defined GPU Scheduling},
    journal = {Proceedings of the 29th Symposium on Operating Systems Principles},
    pages = {595--610},
    year = {2023}
}

@article{nie2021evomoe,
    author = {Xiaonan Nie and Xupeng Miao and Shijie Cao and Lingxiao Ma and Qibin Liu and Jilong Xue and Youshan Miao and Yi Liu and Zhi Yang and Bin Cui},
    title = {Evomoe: An evolutional mixture-of-experts training framework via dense-to-sparse gate},
    journal = {arXiv preprint arXiv:2112.14397},
    year = {2021}
}

@article{nie2023flexmoe,
    author = {Xiaonan Nie and Xupeng Miao and Zilong Wang and Jilong Xue and Lingxiao Ma and Gang Cao},
    title = {FlexMoE: Scaling Large-scale Sparse Pre-trained Model Training via Dynamic Device Placement},
    journal = {Proceedings of the ACM on Management of Data},
    year = {2023}
}



@inproceedings{oliva2017statistical,
  author    = {Junier B Oliva and Barnab{\'a}s P{\'o}czos and Jeff Schneider},
  title     = {The statistical recurrent unit},
  booktitle = {International Conference on Machine Learning},
  pages     = {2671--2680},
  year      = {2017},
  publisher = {PMLR}
}

@article{openai2023gpt,
  author = {OpenAI},
  title = {GPT-4 Technical Report},
  journal = {CoRR},
  volume = {abs/2303.08774},
  year = {2023},
  url = {https://doi.org/10.48550/arXiv.2303.08774}
}

@article{orvieto2023resurrecting,
  author = {Antonio Orvieto and Samuel L Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
  title = {Resurrecting recurrent neural networks for long sequences},
  journal = {arXiv preprint arXiv:2303.06349},
  year = {2023}
}

@article{packer2023memgpt,
  author = {Charles Packer and Vivian Fang and Shishir G Patil and Kevin Lin and Sarah Wooders and Joseph E Gonzalez},
  title = {MemGPT: Towards LLMs as Operating Systems},
  journal = {arXiv preprint arXiv:2310.08560},
  year = {2023}
}

@article{pagliardini2023faster,
  author = {Matteo Pagliardini and Daniele Paliotta and Martin Jaggi and Fran{\c{c}}ois Fleuret},
  title = {Faster Causal Attention Over Large Sequences Through Sparse Flash Attention},
  journal = {arXiv preprint arXiv:2306.01160},
  year = {2023}
}

@article{park2022nuqmm,
  author = {Gunho Park and Baeseong Park and Se Jung Kwon and Byeongwook Kim and Youngjoo Lee and Dongsoo Lee},
  title = {nuqmm: Quantized matmul for efficient inference of large-scale generative language models},
  journal = {arXiv preprint arXiv:2206.09557},
  year = {2022}
}

@article{peng2023rwkv,
  author = {Bo Peng and Eric Alcaide and Quentin Anthony and Alon Albalak and Samuel Arcadinho and Huanqi Cao and Xin Cheng and Michael Chung and Matteo Grella and Kranthi Kiran GV and others},
  title = {RWKV: Reinventing RNNs for the Transformer Era},
  journal = {arXiv preprint arXiv:2305.13048},
  year = {2023}
}

@article{peng2023instruction,
  author = {Baolin Peng and Chunyuan Li and Pengcheng He and Michel Galley and Jianfeng Gao},
  title = {Instruction tuning with gpt-4},
  journal = {arXiv preprint arXiv:2304.03277},
  year = {2023}
}

@article{peng2023chiplet,
  author = {Huwan Peng and Scott Davidson and Richard Shi and Shuaiwen Leon Song and Michael Taylor},
  title = {Chiplet Cloud: Building AI Supercomputers for Serving Large Generative Language Models},
  journal = {arXiv preprint arXiv:2307.02666},
  year = {2023}
}

@article{pope2023efficiently,
  author = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
  title = {Efficiently scaling transformer inference},
  journal = {Proceedings of Machine Learning and Systems},
  volume = {5},
  year = {2023}
}

@inproceedings{press2021train,
  author = {Ofir Press and Noah Smith and Mike Lewis},
  title = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  booktitle = {International Conference on Learning Representations},
  year = {2021}
}

@misc{hybridai,
  author = {Qualcomm},
  title = {The future of AI is hybrid},
  howpublished = {\url{https://www.qualcomm.com/content/dam/qcomm-martech/dm-assets/documents/Whitepaper-The-future-of-AI-is-hybrid-Part-2-Qualcomm-is-uniquely-positioned-to-scale-hybrid-AI.pdf}},
  note = {Accessed on: 2023-11-25}
}

@article{rabe2021self,
  author = {Markus N Rabe and Charles Staats},
  title = {Self-attention Does Not Need O($n^2$) Memory},
  journal = {arXiv preprint arXiv:2112.05682},
  year = {2021}
}

@inproceedings{rajbhandari2022deepspeed,
  author = {Samyam Rajbhandari and Conglong Li and Zhewei Yao and Minjia Zhang and Reza Yazdani Aminabadi and Ammar Ahmad Awan and Jeff Rasley and Yuxiong He},
  title = {Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  booktitle = {International Conference on Machine Learning},
  pages = {18332--18346},
  year = {2022}
}

@inproceedings{ramesh2021zero,
  author = {Aditya Ramesh and Mikhail Pavlov and Gabriel Goh and Scott Gray and Chelsea Voss and Alec Radford and Mark Chen and Ilya Sutskever},
  title = {Zero-shot text-to-image generation},
  booktitle = {International Conference on Machine Learning},
  pages = {8821--8831},
  year = {2021}
}

@inproceedings{reddi2020mlperf,
  author = {Vijay Janapa Reddi and Christine Cheng and David Kanter and Peter Mattson and Guenther Schmuelling and Carole-Jean Wu and Brian Anderson and Maximilien Breughe and Mark Charlebois and William Chou and others},
  title = {MLPerf inference benchmark},
  booktitle = {2020 ACM/IEEE 47th Annual International Symposium on Computer Architecture (ISCA)},
  pages = {446--459},
  year = {2020}
}

@inproceedings{roller2021hash,
  author = {Stephen Roller and Sainbayar Sukhbaatar and Jason Weston and others},
  title = {Hash layers for large sparse models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {17555--17566},
  year = {2021}
}

@article{roy2021efficient,
  author = {Aurko Roy and Mohammad Saffar and Ashish Vaswani and David Grangier},
  title = {Efficient content-based sparse attention with routing transformers},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {9},
  pages = {53--68},
  year = {2021}
}

@inproceedings{sak2014long,
  author = {Hasim Sak and Andrew W Senior and Fran{\c{c}}oise Beaufays},
  title = {Long short-term memory recurrent neural network architectures for large scale acoustic modeling},
  year = {2014}
}

@inproceedings{sampson2022apache,
  author = {Adrian Sampson and Tianqi Chen and Jared Roesch},
  title = {Apache TVM Unity: a vision for the ML software and hardware ecosystem},
  year = {2022}
}

@article{sanh2019distilbert,
  author = {Victor Sanh and Lysandre Debut and Julien Chaumond and Thomas Wolf},
  title = {DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  journal = {arXiv preprint arXiv:1910.01108},
  year = {2019}
}

@article{sanh2020movement,
  author = {Victor Sanh and Thomas Wolf and Alexander Rush},
  title = {Movement pruning: Adaptive sparsity by fine-tuning},
  journal = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {20378--20389},
  year = {2020}
}

@article{santacroce2023matters,
  author = {Michael Santacroce and Zixin Wen and Yelong Shen and Yuanzhi Li},
  title = {What Matters In The Structured Pruning of Generative Language Models?},
  journal = {arXiv preprint arXiv:2302.03773},
  year = {2023}
}

@article{santilli2023accelerating,
  author = {Andrea Santilli and Silvio Severino and Emilian Postolache and Valentino Maiorca and Michele Mancusi and Riccardo Marin and Emanuele Rodol{\`a}},
  title = {Accelerating Transformer Inference for Translation via Parallel Decoding},
  journal = {arXiv preprint arXiv:2305.10427},
  year = {2023}
}

@article{santos2023memory,
  author = {Cicero Nogueira dos Santos and James Lee-Thorp and Isaac Noble and Chung-Ching Chang and David Uthus},
  title = {Memory Augmented Language Models through Mixture of Word Experts},
  journal = {arXiv preprint arXiv:2311.10768},
  year = {2023}
}

@inproceedings{schuster2021consistent,
  author = {Tal Schuster and Adam Fisch and Tommi Jaakkola and Regina Barzilay},
  title = {Consistent Accelerated Inference via Confident Adaptive Transformers},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  pages = {4962--4979},
  year = {2021}
}

@article{shazeer2019fast,
  author = {Noam Shazeer},
  title = {Fast transformer decoding: One write-head is all you need},
  journal = {arXiv preprint arXiv:1911.02150},
  year = {2019}
}

@article{shazeer2017outrageously,
  author = {Noam Shazeer and Azalia Mirhoseini and Krzysztof Maziarz and Andy Davis and Quoc Le and Geoffrey Hinton and Jeff Dean},
  title = {Outrageously large neural networks: The sparsely-gated mixture-of-experts layer},
  journal = {arXiv preprint arXiv:1701.06538},
  year = {2017}
}

@article{shen2023efficient,
  author = {Haihao Shen and Hanwen Chang and Bo Dong and Yu Luo and Hengyu Meng},
  title = {Efficient LLM Inference on CPUs},
  journal = {arXiv preprint arXiv:2311.00502},
  year = {2023}
}

@article{sheng2023s,
  author = {Ying Sheng and Shiyi Cao and Dacheng Li and Coleman Hooper and Nicholas Lee and Shuo Yang and Christopher Chou and Banghua Zhu and Lianmin Zheng and Kurt Keutzer and others},
  title = {S-LoRA: Serving Thousands of Concurrent LoRA Adapters},
  journal = {arXiv preprint arXiv:2311.03285},
  year = {2023}
}

@article{sheng2023high,
  author = {Ying Sheng and Lianmin Zheng and Binhang Yuan and Zhuohan Li and Max Ryabinin and Beidi Chen and Percy Liang and Christopher R{\'e} and Ion Stoica and Ce Zhang},
  title = {FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU},
  journal = {International Conference on Machine Learning},
  year = {2023}
}

@inproceedings{shi2017speeding,
  author = {Xing Shi and Kevin Knight},
  title = {Speeding up neural machine translation decoding by shrinking run-time vocabulary},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
  pages = {574--579},
  year = {2017}
}

@inproceedings{shi2023welder,
  author = {Yining Shi and Zhi Yang and Jilong Xue and Lingxiao Ma and Yuqing Xia and Ziming Miao and Yuxiao Guo and Fan Yang and Lidong Zhou},
  title = {Welder: Scheduling Deep Learning Memory Access via Tile-graph},
  booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages = {701--718},
  year = {2023}
}

@inproceedings{shoeybi2019megatron,
  author = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  title = {Megatron-lm: Training multi-billion parameter language models using model parallelism},
  booktitle = {arXiv preprint arXiv:1909.08053},
  year = {2019}
}

@article{song2023powerinfer,
  author = {Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},
  title = {PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU},
  journal = {arXiv preprint arXiv:2312.12456},
  year = {2023}
}

@article{spector2023accelerating,
  author = {Benjamin Spector and Chris Re},
  title = {Accelerating llm inference with staged speculative decoding},
  journal = {arXiv preprint arXiv:2308.04623},
  year = {2023}
}

@inproceedings{stern2018blockwise,
  author = {Mitchell Stern and Noam Shazeer and Jakob Uszkoreit},
  title = {Blockwise parallel decoding for deep autoregressive models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {31},
  year = {2018}
}

@inproceedings{su2021roformer,
  author = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
  title = {Roformer: Enhanced transformer with rotary position embedding},
  booktitle = {arXiv preprint arXiv:2104.09864},
  year = {2021}
}

@article{sun2023simple,
  author = {Mingjie Sun and Zhuang Liu and Anna Bair and J Zico Kolter},
  title = {A Simple and Effective Pruning Approach for Large Language Models},
  journal = {arXiv preprint arXiv:2306.11695},
  year = {2023}
}

@article{sun2019patient,
  author = {Siqi Sun and Yu Cheng and Zhe Gan and Jingjing Liu},
  title = {Patient Knowledge Distillation for BERT Model Compression},
  journal = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages = {4323--4332},
  year = {2019}
}

@inproceedings{sun2022simple,
  author = {Tianxiang Sun and Xiangyang Liu and Wei Zhu and Zhichao Geng and Lingling Wu and Yilong He and Yuan Ni and Guotong Xie and Xuanjing Huang and Xipeng Qiu},
  title = {A simple hash-based early exiting approach for language understanding and generation},
  booktitle = {arXiv preprint arXiv:2203.01670},
  year = {2022}
}

@article{sun2023retentive,
  author = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  title = {Retentive Network: A Successor to Transformer for Large Language Models},
  journal = {arXiv preprint arXiv:2307.08621},
  year = {2023}
}

@inproceedings{sun2023spectr,
  author = {Ziteng Sun and Ananda Theertha Suresh and Jae Hun Ro and Ahmad Beirami and Himanshu Jain and Felix Yu and Michael Riley and Sanjiv Kumar},
  title = {Spectr: Fast speculative decoding via optimal transport},
  booktitle = {Workshop on Efficient Systems for Foundation Models@ ICML2023},
  year = {2023}
}

@inproceedings{tang2023fusionai,
  author = {Zhenheng Tang and Yuxin Wang and Xin He and Longteng Zhang and Xinglin Pan and Qiang Wang and Rongfei Zeng and Kaiyong Zhao and Shaohuai Shi and Bingsheng He and others},
  title = {FusionAI: Decentralized Training and Deploying LLMs with Massive Consumer-Level GPUs},
  booktitle = {arXiv preprint arXiv:2309.01172},
  year = {2023}
}

@inproceedings{taori2023stanford,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori Hashimoto},
  title = {Stanford alpaca: An instruction-following llama model},
  booktitle = {arXiv preprint arXiv:2303.06349},
  year = {2023}
}

@inproceedings{tay2020sparse,
  author = {Yi Tay and Dara Bahri and Liu Yang and Donald Metzler and Da-Cheng Juan},
  title = {Sparse sinkhorn attention},
  booktitle = {International Conference on Machine Learning},
  pages = {9438--9447},
  year = {2020}
}

@article{DBLP:journals/csur/TayDBM23,
  author = {Yi Tay and Mostafa Dehghani and Dara Bahri and Donald Metzler},
  title = {Efficient Transformers: A Survey},
  journal = {ACM Comput. Surv.},
  volume = {55},
  number = {6},
  pages = {109:1--109:28},
  year = {2023},
  url = {https://doi.org/10.1145/3530811}
}

@misc{decilm,
  author = {DeciAI Research Team},
  title = {DeciLM 6B},
  howpublished = {Hugging Face},
  url = {https://huggingface.co/Deci/DeciLM-6b}
}

@article{mlc-llm,
  author = {MLC team},
  title = {MLC-LLM},
  journal = {GitHub},
  url = {https://github.com/mlc-ai/mlc-llm},
  note = {Commit: 3358029, Accessed on: 2023-11-25}
}

@inproceedings{teerapittayanon2016branchynet,
  author = {Surat Teerapittayanon and Bradley McDanel and Hsiang-Tsung Kung},
  title = {Branchynet: Fast inference via early exiting from deep neural networks},
  booktitle = {2016 23rd International Conference on Pattern Recognition (ICPR)},
  pages = {2464--2469},
  year = {2016},
  publisher = {IEEE}
}

@inproceedings{tillet2019triton,
  author = {Philippe Tillet and Hsiang-Tsung Kung and David Cox},
  title = {Triton: an intermediate language and compiler for tiled neural network computations},
  booktitle = {Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages},
  pages = {10--19},
  year = {2019}
}

@article{tolstikhin2021mlp,
  author = {Ilya O Tolstikhin and Neil Houlsby and Alexander Kolesnikov and Lucas Beyer and Xiaohua Zhai and Thomas Unterthiner and Jessica Yung and Andreas Steiner and Daniel Keysers and Jakob Uszkoreit and others},
  title = {MLP-Mixer: An all-MLP architecture for vision},
  journal = {Advances in Neural Information Processing Systems},
  volume = {34},
  pages = {24261--24272},
  year = {2021}
}

@article{tornede2023automl,
  author = {Alexander Tornede and Difan Deng and Theresa Eimer and Joseph Giovanelli and Aditya Mohan and Tim Ruhkopf and Sarah Segel and Daphne Theodorakopoulos and Tanja Tornede and Henning Wachsmuth and others},
  title = {AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks},
  journal = {arXiv preprint arXiv:2306.08107},
  year = {2023}
}

@article{touvron2023llama,
  author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and others},
  title = {Llama 2: Open foundation and fine-tuned chat models},
  journal = {arXiv preprint arXiv:2307.09288},
  year = {2023}
}

@inproceedings{treviso2023efficient,
  author = {Marcos Treviso and Ji-Ung Lee and Tianchu Ji and Betty van Aken and Qingqing Cao and Manuel R Ciosici and Michael Hassid and Kenneth Heafield and Sara Hooker and Colin Raffel and others},
  title = {Efficient methods for natural language processing: A survey},
  booktitle = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {826--860},
  year = {2023}
}

@article{tri2023flash,
  author = {Francisco Massa and Grigory Sizov and Tri Dao and Daniel Haziza},
  title = {Flash-Decoding for long-context inference},
  journal = {arXiv preprint arXiv:2304.10592},
  year = {2023},
  url = {https://pytorch.org/blog/flash-decoding/}
}

@inproceedings{unger2022unity,
  author = {Colin Unger and Zhihao Jia and Wei Wu and Sina Lin and Mandeep Baines and Carlos Efrain Quintero Narvaez and Vinay Ramakrishnaiah and Nirmal Prajapati and Pat McCormick and Jamaludin Mohd-Yusof and others},
  title = {Unity: Accelerating DNN training through joint optimization of algebraic transformations and parallelization},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {267--284},
  year = {2022}
}

@article{valicenti2023mini,
  author = {Tim Valicenti and Justice Vidal and Ritik Patnaik},
  title = {Mini-GPTs: Efficient Large Language Models through Contextual Pruning},
  journal = {arXiv preprint arXiv:2312.12682},
  year = {2023}
}

@inproceedings{van1997summa,
  author = {Robert A Van De Geijn and Jerrell Watts},
  title = {SUMMA: Scalable universal matrix multiplication algorithm},
  booktitle = {Concurrency: Practice and Experience},
  volume = {9},
  number = {4},
  pages = {255--274},
  year = {1997}
}

@article{vaswani2017attention,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N Gomez and ukasz Kaiser and Illia Polosukhin},
  title = {Attention is all you need},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  year = {2017}
}

@inproceedings{wang2020linformer,
  author = {Sinong Wang and Belinda Z Li and Madian Khabsa and Han Fang and Hao Ma},
  title = {Linformer: Self-attention with linear complexity},
  booktitle = {arXiv preprint arXiv:2006.04768},
  year = {2020}
}

@inproceedings{wang2020minilm,
  author = {Wenhui Wang and Furu Wei and Li Dong and Hangbo Bao and Nan Yang and Ming Zhou},
  title = {Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {5776--5788},
  year = {2020}
}

@inproceedings{wang2020lightseq,
  author = {Xiaohui Wang and Ying Xiong and Yang Wei and Mingxuan Wang and Lei Li},
  title = {LightSeq: A high performance inference library for transformers},
  booktitle = {arXiv preprint arXiv:2010.13887},
  year = {2020}
}

@inproceedings{wang2023tabi,
  author = {Yiding Wang and Kai Chen and Haisheng Tan and Kun Guo},
  title = {Tabi: An Efficient Multi-Level Inference System for Large Language Models},
  booktitle = {Proceedings of the Eighteenth European Conference on Computer Systems},
  pages = {233--248},
  year = {2023}
}

@inproceedings{wei2022chain,
  author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Fei Xia and Ed Chi and Quoc V Le and Denny Zhou and others},
  title = {Chain-of-thought prompting elicits reasoning in large language models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24824--24837},
  year = {2022}
}

@inproceedings{weng2022mlaas,
  author = {Qizhen Weng and Wencong Xiao and Yinghao Yu and Wei Wang and Cheng Wang and Jian He and Yong Li and Liping Zhang and Wei Lin and Yu Ding},
  title = {MLaaS in the wild: Workload analysis and scheduling in Large-Scale heterogeneous GPU clusters},
  booktitle = {19th USENIX Symposium on Networked Systems Design and Implementation (NSDI 22)},
  pages = {945--960},
  year = {2022}
}

@article{workshop2022bloom,
  author = {BigScience Workshop and Teven Le Scao and Angela Fan and Christopher Akiki and Ellie Pavlick and Suzana Ili{\`c} and Daniel Hesslow and Roman Castagn{\'e} and Alexandra Sasha Luccioni and Fran{\c{c}}ois Yvon},
  title = {Bloom: A 176b-parameter open-access multilingual language model},
  journal = {arXiv preprint arXiv:2211.05100},
  year = {2022}
}

@inproceedings{wu2023fast,
  author = {Bingyang Wu and Yinmin Zhong and Zili Zhang and Gang Huang and Xuanzhe Liu and Xin Jin},
  title = {Fast Distributed Inference Serving for Large Language Models},
  booktitle = {arXiv preprint arXiv:2305.05920},
  year = {2023}
}

@inproceedings{wu2021tentrans,
  author = {Kaixin Wu and Bojie Hu and Qi Ju},
  title = {TenTrans High-Performance Inference Toolkit for WMT2021 Efficiency Task},
  booktitle = {Proceedings of the Sixth Conference on Machine Translation},
  pages = {795--798},
  year = {2021}
}

@inproceedings{wu2022speeding,
  author = {Kaixin Wu and Yue Zhang and Bojie Hu and Tong Zhang},
  title = {Speeding up Transformer Decoding via an Attention Refinement Network},
  booktitle = {Proceedings of the 29th International Conference on Computational Linguistics},
  pages = {5109--5118},
  year = {2022}
}

@inproceedings{wu2023pytorch,
  author = {Peng Wu},
  title = {PyTorch 2.0: The Journey to Bringing Compiler Technologies to the Core of PyTorch (Keynote)},
  booktitle = {Proceedings of the 21st ACM/IEEE International Symposium on Code Generation and Optimization},
  pages = {1--1},
  year = {2023}
}

@inproceedings{wu2023autogen,
  author = {Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li Jiang and Xiaoyun Zhang and Chi Wang},
  title = {Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  booktitle = {arXiv preprint arXiv:2308.08155},
  year = {2023}
}

@article{wu2023understanding,
  author = {Xiaoxia Wu and Cheng Li and Reza Yazdani Aminabadi and Zhewei Yao and Yuxiong He},
  title = {Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases},
  journal = {arXiv preprint arXiv:2301.12017},
  year = {2023}
}

@article{xia2023flash,
  author = {Haojun Xia and Zhen Zheng and Yuchao Li and Donglin Zhuang and Zhongzhu Zhou and Xiafei Qiu and Yong Li and Wei Lin and Shuaiwen Leon Song},
  title = {Flash-LLM: Enabling Cost-Effective and Highly-Efficient Large Generative Model Inference with Unstructured Sparsity},
  journal = {arXiv preprint arXiv:2309.10285},
  year = {2023}
}

@article{xiao2022smoothquant,
  author = {Guangxuan Xiao and Ji Lin and Mickael Seznec and Julien Demouth and Song Han},
  title = {Smoothquant: Accurate and efficient post-training quantization for large language models},
  journal = {arXiv preprint arXiv:2211.10438},
  year = {2022}
}

@article{xiao2023efficient,
  author = {Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  title = {Efficient Streaming Language Models with Attention Sinks},
  journal = {arXiv preprint arXiv:2309.17453},
  year = {2023}
}

@inproceedings{xiao2023survey,
  author = {Yisheng Xiao and Lijun Wu and Junliang Guo and Juntao Li and Min Zhang and Tao Qin and Tie-yan Liu},
  title = {A survey on non-autoregressive generation for neural machine translation and beyond},
  booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year = {2023}
}

@inproceedings{xin2020deebert,
  author = {Ji Xin and Raphael Tang and Jaejun Lee and Yaoliang Yu and Jimmy Lin},
  title = {DeeBERT: Dynamic Early Exiting for Accelerating BERT Inference},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  pages = {2246--2251},
  year = {2020}
}

@article{xu2023wizardlm,
  author = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Daxin Jiang},
  title = {Wizardlm: Empowering large language models to follow complex instructions},
  journal = {arXiv preprint arXiv:2304.12244},
  year = {2023}
}

@article{xu2023llmcad,
  author = {Daliang Xu and Wangsong Yin and Xin Jin and Ying Zhang and Shiyun Wei and Mengwei Xu and Xuanzhe Liu},
  title = {LLMCad: Fast and Scalable On-device Large Language Model Inference},
  journal = {arXiv preprint arXiv:2309.04255},
  year = {2023}
}

@article{xu2023retrieval,
  author = {Peng Xu and Wei Ping and Xianchao Wu and Lawrence McAfee and Chen Zhu and Zihan Liu and Sandeep Subramanian and Evelina Bakhturina and Mohammad Shoeybi and Bryan Catanzaro},
  title = {Retrieval meets Long Context Large Language Models},
  journal = {arXiv preprint arXiv:2310.03025},
  year = {2023}
}

@article{xu2023compress,
  author = {Zhaozhuo Xu and Zirui Liu and Beidi Chen and Yuxin Tang and Jue Wang and Kaixiong Zhou and Xia Hu and Anshumali Shrivastava},
  title = {Compress, Then Prompt: Improving Accuracy-Efficiency Trade-off of LLM Inference with Transferable Prompt},
  journal = {arXiv preprint arXiv:2305.11186},
  year = {2023}
}

@inproceedings{yang2023baichuan,
  author = {Aiyuan Yang and Bin Xiao and Bingning Wang and Borong Zhang and Chao Yin and Chenxu Lv and Da Pan and Dian Wang and Dong Yan and Fan Yang and others},
  title = {Baichuan 2: Open large-scale language models},
  booktitle = {arXiv preprint arXiv:2309.10305},
  year = {2023}
}

@article{yang2023inference,
  author = {Nan Yang and Tao Ge and Liang Wang and Binxing Jiao and Daxin Jiang and Linjun Yang and Rangan Majumder and Furu Wei},
  title = {Inference with reference: Lossless acceleration of large language models},
  journal = {arXiv preprint arXiv:2304.04487},
  year = {2023}
}

@article{yang2023predictive,
  author = {Seongjun Yang and Gibbeum Lee and Jaewoong Cho and Dimitris Papailiopoulos and Kangwook Lee},
  title = {Predictive Pipelined Decoding: A Compute-Latency Trade-off for Exact LLM Decoding},
  journal = {arXiv preprint arXiv:2307.05908},
  year = {2023}
}

@article{yao2023comprehensive,
  author = {Zhewei Yao and Cheng Li and Xiaoxia Wu and Stephen Youn and Yuxiong He},
  title = {A comprehensive study on post-training quantization for large language models},
  journal = {arXiv preprint arXiv:2303.08302},
  year = {2023}
}

@article{yao2022zeroquant,
  author = {Zhewei Yao and Reza Yazdani Aminabadi and Minjia Zhang and Xiaoxia Wu and Conglong Li and Yuxiong He},
  title = {Zeroquant: Efficient and affordable post-training quantization for large-scale transformers},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {27168--27183},
  year = {2022}
}

@article{ye2021tr,
  author = {Deming Ye and Yankai Lin and Yufei Huang and Maosong Sun},
  title = {TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference},
  journal = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages = {5798--5809},
  year = {2021}
}

@inproceedings{ye2023sparsetir,
  author = {Zihao Ye and Ruihang Lai and Junru Shao and Tianqi Chen and Luis Ceze},
  title = {SparseTIR: Composable abstractions for sparse compilation in deep learning},
  booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages = {660--678},
  year = {2023}
}

@inproceedings{yemme2023scalable,
  author = {Anil Yemme and Shayan Srinivasa Garani},
  title = {A Scalable GPT-2 Inference Hardware Architecture on FPGA},
  booktitle = {2023 International Joint Conference on Neural Networks (IJCNN)},
  pages = {1--8},
  year = {2023}
}

@inproceedings{yu2022orca,
  author = {Gyeong-In Yu and Joo Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
  title = {Orca: A Distributed Serving System for Transformer-Based Generative Models},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {521--538},
  year = {2022}
}

@inproceedings{yu2022metaformer,
  author = {Weihao Yu and Mi Luo and Pan Zhou and Chenyang Si and Yichen Zhou and Xinchao Wang and Jiashi Feng and Shuicheng Yan},
  title = {Metaformer is actually what you need for vision},
  booktitle = {Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages = {10819--10829},
  year = {2022}
}

@article{yuan2023rptq,
  author = {Zhihang Yuan and Lin Niu and Jiawei Liu and Wenyu Liu and Xinggang Wang and Yuzhang Shang and Guangyu Sun and Qiang Wu and Jiaxiang Wu and Bingzhe Wu},
  title = {RPTQ: Reorder-based Post-training Quantization for Large Language Models},
  journal = {arXiv preprint arXiv:2304.01089},
  year = {2023}
}

@article{yue2023large,
  author = {Murong Yue and Jie Zhao and Min Zhang and Du Liang and Ziyu Yao},
  title = {LARGE LANGUAGE MODEL CASCADES WITH MIX-TURE OF THOUGHT REPRESENTATIONS FOR COST-EFFICIENT REASONING},
  journal = {arXiv preprint arXiv:2310.03094},
  year = {2023}
}

@inproceedings{zaheer2020big,
  author = {Manzil Zaheer and Guru Guruganesh and Kumar Avinava Dubey and Joshua Ainslie and Chris Alberti and Santiago Ontanon and Philip Pham and Anirudh Ravula and Qifan Wang and Li Yang and others},
  title = {Big bird: Transformers for longer sequences},
  booktitle = {Advances in neural information processing systems},
  volume = {33},
  pages = {17283--17297},
  year = {2020}
}

@article{zeng2022glm,
  author = {Aohan Zeng and Xiao Liu and Zhengxiao Du and Zihan Wang and Hanyu Lai and Ming Ding and Zhuoyi Yang and Yifan Xu and Wendi Zheng and Xiao Xia and others},
  title = {Glm-130b: An open bilingual pre-trained model},
  journal = {arXiv preprint arXiv:2210.02414},
  year = {2022}
}

@article{zeng2023learning,
  author = {Dewen Zeng and Nan Du and Tao Wang and Yuanzhong Xu and Tao Lei and Zhifeng Chen and Claire Cui},
  title = {Learning to Skip for Language Modeling},
  journal = {arXiv preprint arXiv:2311.15436},
  year = {2023}
}

@article{zhai2021attention,
  author = {Shuangfei Zhai and Walter Talbott and Nitish Srivastava and Chen Huang and Hanlin Goh and Ruixiang Zhang and Josh Susskind},
  title = {An attention free transformer},
  journal = {arXiv preprint arXiv:2105.14103},
  year = {2021}
}

@inproceedings{zhai2023bytetransformer,
  author = {Yujia Zhai and Chengquan Jiang and Leyuan Wang and Xiaoying Jia and Shang Zhang and Zizhong Chen and Xin Liu and Yibo Zhu},
  title = {Bytetransformer: A high-performance transformer boosted for variable-length inputs},
  booktitle = {2023 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages = {344--355},
  year = {2023}
}

@inproceedings{zhan2023depa,
  author = {Jiaao Zhan and Qian Chen and Boxing Chen and Wen Wang and Yu Bai and Yang Gao},
  title = {DePA: Improving Non-autoregressive Translation with Dependency-Aware Decoder},
  booktitle = {Proceedings of the 20th International Conference on Spoken Language Translation (IWSLT 2023)},
  pages = {478--490},
  year = {2023}
}

@inproceedings{zhang2019mark,
  author = {Chengliang Zhang and Minchen Yu and Wei Wang and Feng Yan},
  title = {MArk: Exploiting Cloud Services for Cost-Effective, SLO-Aware Machine Learning Inference Serving},
  booktitle = {2019 USENIX Annual Technical Conference (USENIX ATC 19)},
  pages = {1049--1062},
  year = {2019}
}

@article{zhang2023draft,
  author = {Jun Zhang and Jue Wang and Huan Li and Lidan Shou and Ke Chen and Gang Chen and Sharad Mehrotra},
  title = {Draft \& Verify: Lossless Large Language Model Acceleration via Self-Speculative Decoding},
  journal = {arXiv preprint arXiv:2309.08168},
  year = {2023}
}

@article{zhang2023dissecting,
  author = {Longteng Zhang and Xiang Liu and Zeyu Li and Xinglin Pan and Peijie Dong and Ruibo Fan and Rui Guo and Xin Wang and Qiong Luo and Shaohuai Shi},
  title = {Dissecting the Runtime Performance of the Training, Fine-tuning, and Inference of Large Language Models},
  journal = {arXiv preprint arXiv:2311.03687},
  year = {2023}
}

@article{zhang2023latticegen,
  author = {Mengke Zhang and Tianxing He and Tianle Wang and Fatemehsadat Mireshghallah and Binyi Chen and Hao Wang and Yulia Tsvetkov},
  title = {LatticeGen: A Cooperative Framework which Hides Generated Text in a Lattice for Privacy-Aware Generation on Cloud},
  journal = {arXiv preprint arXiv:2309.17157},
  year = {2023}
}

@article{zhang2023efficient,
  author = {Qingru Zhang and Dhananjay Ram and Cole Hawkins and Sheng Zha},
  title = {Efficient Long-Range Transformers: You Need to Attend More, but Not Necessarily at Every Layer},
  journal = {Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages = {2775--2786},
  year = {2023}
}

@article{zhang2022opt,
  author = {Susan Zhang and Stephen Roller and Naman Goyal and Mikel Artetxe and Moya Chen and Shuohui Chen and Christopher Dewan and Mona Diab and Xian Li and Xi Victoria Lin},
  title = {Opt: Open pre-trained transformer language models},
  journal = {arXiv preprint arXiv:2205.01068},
  year = {2022}
}

@article{zhang2023h,
  author = {Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher R and Clark Barrett},
  title = {H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  journal = {arXiv preprint arXiv:2306.14048},
  year = {2023}
}

@article{zhao2023atom,
  author = {Yilong Zhao and Chien-Yu Lin and Kan Zhu and Zihao Ye and Lequn Chen and Size Zheng and Luis Ceze and Arvind Krishnamurthy and Tianqi Chen and Baris Kasikci},
  title = {Atom: Low-bit Quantization for Efficient and Accurate LLM Serving},
  journal = {arXiv preprint arXiv:2310.19102},
  year = {2023}
}

@inproceedings{zheng2022alpa,
  author = {Lianmin Zheng and Zhuohan Li and Hao Zhang and Yonghao Zhuang and Zhifeng Chen and Yanping Huang and Yida Wang and Yuanzhong Xu and Danyang Zhuo and Eric P Xing},
  title = {Alpa: Automating inter-and Intra-Operator parallelism for distributed deep learning},
  booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages = {559--578},
  year = {2022}
}

@inproceedings{zheng2023einnet,
  author = {Liyan Zheng and Haojie Wang and Jidong Zhai and Muyan Hu and Zixuan Ma and Tuowei Wang and Shuhong Huang and Xupeng Miao and Shizhi Tang and Kezhao Huang and others},
  title = {EINNET: Optimizing Tensor Programs with Derivation-Based Transformations},
  booktitle = {17th USENIX Symposium on Operating Systems Design and Implementation (OSDI 23)},
  pages = {739--755},
  year = {2023}
}

@inproceedings{zheng2023pit,
  author = {Ningxin Zheng and Huiqiang Jiang and Quanlu Zhang and Zhenhua Han and Lingxiao Ma and Yuqing Yang and Fan Yang and Chengruidong Zhang and Lili Qiu and Mao Yang and others},
  title = {PIT: Optimization of Dynamic Sparse Deep Learning Models via Permutation Invariant Transformation},
  booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
  pages = {331--347},
  year = {2023}
}

@article{zhou2021informer,
  author = {Haoyi Zhou and Shanghang Zhang and Jieqi Peng and Shuai Zhang and Jianxin Li and Hui Xiong and Wancai Zhang},
  title = {Informer: Beyond efficient transformer for long sequence time-series forecasting},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  pages = {11106--11115},
  year = {2021}
}

@inproceedings{zhou2022transpim,
  author = {Minxuan Zhou and Weihong Xu and Jaeyoung Kang and Tajana Rosing},
  title = {Transpim: A memory-based acceleration via software-hardware co-design for transformer},
  booktitle = {2022 IEEE International Symposium on High-Performance Computer Architecture (HPCA)},
  pages = {1071--1085},
  year = {2022}
}

@inproceedings{zhou2020bert,
  author = {Wangchunshu Zhou and Canwen Xu and Tao Ge and Julian McAuley and Ke Xu and Furu Wei},
  title = {Bert loses patience: Fast and robust inference with early exit},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {33},
  pages = {18330--18341},
  year = {2020}
}

@inproceedings{zhou2022mixture,
  author = {Yanqi Zhou and Tao Lei and Hanxiao Liu and Nan Du and Yanping Huang and Vincent Zhao and Andrew M Dai and Quoc V Le and James Laudon},
  title = {Mixture-of-experts with expert choice routing},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {7103--7114},
  year = {2022}
}

@article{zheng2023distillspec,
  author = {Yongchao Zhou and Kaifeng Lyu and Ankit Singh Rawat and Aditya Krishna Menon and Afshin Rostamizadeh and Sanjiv Kumar and Jean-Franois Kagy and Rishabh Agarwal},
  title = {DistillSpec: Improving Speculative Decoding via Knowledge Distillation},
  journal = {arXiv preprint arXiv:2310.08461},
  year = {2023}
}

@inproceedings{zhou2022pets,
  author = {Zhe Zhou and Xuechao Wei and Jiejing Zhang and Guangyu Sun},
  title = {PetS: A Unified Framework for Parameter-Efficient Transformers Serving},
  booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
  pages = {489--504},
  year = {2022}
}

@article{zhu2023optimal,
  author = {Banghua Zhu and Ying Sheng and Lianmin Zheng and Clark Barrett and Michael I Jordan and Jiantao Jiao},
  title = {On Optimal Caching and Model Multiplexing for Large Model Inference},
  journal = {arXiv preprint arXiv:2306.02003},
  year = {2023}
}

@inproceedings{zhu2023minigpt,
  author = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  title = {Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  booktitle = {arXiv preprint arXiv:2304.10592},
  year = {2023}
}

@article{zhu2023survey,
  author = {Xunyu Zhu and Jian Li and Yong Liu and Can Ma and Weiping Wang},
  title = {A survey on model compression for large language models},
  journal = {arXiv preprint arXiv:2308.07633},
  year = {2023}
}

@article{zxhang2023falcon,
  author = {Yoshua X ZXhang and Yann M Haxo and Ying X Mat},
  title = {Falcon LLM: A New Frontier in Natural Language Processing},
  journal = {AC Investment Research Journal},
  volume = {220},
  number = {44},
  year = {2023}
}










































