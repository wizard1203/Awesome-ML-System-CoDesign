@article{tan2024teola,
  title={Teola: Towards end-to-end optimization of llm-based applications},
  author={Tan, Xin and Jiang, Yimin and Yang, Yitao and Xu, Hong},
  journal={arXiv preprint arXiv:2407.00326},
  year={2024}
}

@misc{llamaindex,
  title = {LlamaIndex},
  year = {2022},
  url = {https://github.com/jerryjliu/llama_index}
}

@misc{promptflow,
  title = {Promptflow},
  year = {2023},
  url = {https://https://github.com/microsoft/promptflow}
}

@misc{autogpt,
  title = {Autogpt},
  year = {2024},
  url = {https://github.com/Significant-Gravitas/AutoGPT}
}

@misc{bing,
  title = {Bing Copilot},
  year = {2024},
  url = {https://www.bing.com/chat}
}

@misc{characterai,
  title = {Character.ai},
  year = {2024},
  url = {https://character.ai/}
}

@misc{contextual-retrieval,
  title = {contextual-retrieval},
  year = {2024},
  url = {https://www.anthropic.com/news/contextual-retrieval}
}

@misc{Fastapi,
  title = {Fastapi},
  year = {2024},
  url = {https://fastapi.tiangolo.com/}
}

@misc{Finqabench,
  title = {Finqabench Dataset},
  year = {2024},
  url = {https://huggingface.co/datasets/lighthouzai/finqabench}
}

@misc{googlesearch,
  title = {Google custom search},
  year = {2024},
  url = {https://programmablesearchengine.google.com/}
}

@misc{azurerag,
  title = {Gpt-rag},
  year = {2024},
  url = {https://github.com/Azure/GPT-RAG}
}

@misc{haystack,
  title = {haystack},
  year = {2024},
  url = {https://github.com/deepset-ai/haystack}
}

@misc{langchain,
  title = {Langchain},
  year = {2024},
  url = {https://github.com/langchain-ai/langchain}
}

@misc{langgraph,
  title = {LangGraph},
  year = {2024},
  url = {https://python.langchain.com/docs/langgraph/}
}

@misc{lazyllm,
  title = {Lazyllm},
  year = {2024},
  url = {https://github.com/LazyAGI/LazyLLM}
}

@misc{alibaba_llmapp_observation,
  title = {Observability of llm applications: Exploration and practice from the perspective of trace},
  year = {2024},
  url = {https://www.alibabacloud.com/blog/observability-of-llm-applications-exploration-and-practice-from-the-perspective-of-trace_601604}
}

@misc{openaifunc,
  title = {Openai function calling},
  year = {2024},
  url = {https://platform.openai.com/docs/guides/function-calling}
}

@misc{pairag,
  title = {Pairag},
  year = {2024},
  url = {https://github.com/aigc-apps/PAI-RAG}
}

@misc{perplexity,
  title = {Perplexity ai},
  year = {2024},
  url = {https://www.perplexity.ai/}
}

@misc{pgvector,
  title = {Pgvector},
  year = {2024},
  url = {https://github.com/pgvector/pgvector}
}

@misc{postgresql,
  title = {Postgresql},
  year = {2024},
  url = {https://www.postgresql.org/}
}

@misc{privatellm,
  title = {Privatellm},
  year = {2024},
  url = {https://privatellm.app/en}
}

@misc{tritonserver,
  title = {Triton inference server},
  year = {2024},
  url = {https://github.com/triton-inference-server}
}

@inproceedings{tensorflow,
  author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek~G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title = {TensorFlow: A system for Large-Scale machine learning},
  booktitle = {Proc.~USENIX OSDI},
  year = {2016}
}

@article{agrawal2024taming,
  author = {Amey Agrawal and Nitin Kedia and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav~S Gulavani and Alexey Tumanov and Ramachandran Ramjee},
  title = {Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.02310}
}

@article{agrawal2023sarathi,
  author = {Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav~S Gulavani and Ramachandran Ramjee},
  title = {Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.16369}
}

@inproceedings{bang2023gptcache,
  author = {Fu~Bang},
  title = {Gptcache: An open-source semantic cache for llm applications enabling faster answers and cost savings},
  booktitle = {Proc.~the 3rd Workshop for Natural Language Processing Open Source Software},
  year = {2023}
}

@inproceedings{bauer2012legion,
  author = {Michael Bauer and Sean Treichler and Elliott Slaughter and Alex Aiken},
  title = {Legion: Expressing locality and independence with logical regions},
  booktitle = {Proc. IEEE SC},
  year = {2012}
}

@inproceedings{webquestion,
  author = {Jonathan Berant and Andrew Chou and Roy Frostig and Percy Liang},
  title = {Semantic parsing on Freebase from question-answer pairs},
  booktitle = {Proc.~EMNLP},
  year = {2013},
  month = {October}
}

@inproceedings{crankshaw2017clipper,
  author = {Daniel Crankshaw and Xin Wang and Guilio Zhou and Michael~J Franklin and Joseph~E Gonzalez and Ion Stoica},
  title = {Clipper: A Low-Latency online prediction serving system},
  booktitle = {Proc.~USENIX NSDI},
  year = {2017}
}

@inproceedings{dao2022flashattention,
  author = {Tri Dao and Dan Fu and Stefano Ermon and Atri Rudra and Christopher R{\'e}},
  title = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  booktitle = {Proc.~NeurIPS},
  year = {2022}
}

@inproceedings{devlin2018bert,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  booktitle = {Proc.~ACL},
  year = {2018}
}

@article{gao2023retrievalsurvey,
  author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi~Dai and Jiawei Sun and Haofen Wang},
  title = {Retrieval-augmented generation for large language models: A survey},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.10997}
}

@article{gim2023promptcache,
  author = {In~Gim and Guojun Chen and Seung-seob Lee and Nikhil Sarda and Anurag Khandelwal and Lin Zhong},
  title = {Prompt cache: Modular attention reuse for low-latency inference},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.04934}
}

@inproceedings{gog2015musketeer,
  author = {Ionel Gog and Malte Schwarzkopf and Natacha Crooks and Matthew~P Grosvenor and Allen Clement and Steven Hand},
  title = {Musketeer: all for one, one for all in data processing systems},
  booktitle = {Proc. ACM Eurosys},
  year = {2015}
}

@inproceedings{gujarati2020clockwork,
  author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
  title = {Serving DNNs like clockwork: Performance predictability from the bottom up},
  booktitle = {Proc.~USENIX OSDI},
  year = {2020}
}

@inproceedings{hong2023flashdecoding++,
  author = {Ke~Hong and Guohao Dai and Jiaming Xu and Qiuli Mao and Xiuhong Li and Jun Liu and Kangdi Chen and Hanyu Dong and Yu~Wang},
  title = {Flashdecoding++: Faster large language model inference on gpus},
  booktitle = {Proc.~Machine Learning and Systems},
  year = {2023}
}

@article{hong2024data,
  author = {Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li~Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},
  title = {Data interpreter: An llm agent for data science},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2402.18679}
}

@article{hong2023metagpt,
  author = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka~Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
  title = {Metagpt: Meta programming for a multi-agent collaborative framework},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.00352}
}

@article{hu2024inferencewithoutinterference,
  author = {Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa~Wang and Yungang Bao and et~al.},
  title = {Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.11181}
}

@article{huang2023Hallucination,
  author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
  title = {A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.05232}
}

@article{huang2024tool,
  author = {Zhongzhen Huang and Kui Xue and Yongqi Fan and Linjie Mu and Ruoyu Liu and Tong Ruan and Shaoting Zhang and Xiaofan Zhang},
  title = {Tool calling: Enhancing medication consultation via retrieval-augmented large language models},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.17897}
}

@inproceedings{isard2007dryad,
  author = {Michael Isard and Mihai Budiu and Yuan Yu and Andrew Birrell and Dennis Fetterly},
  title = {Dryad: distributed data-parallel programs from sequential building blocks},
  booktitle = {Proc.~ACM Eurosys},
  year = {2007}
}

@article{jagerman2023queryexpansion,
  author = {Rolf Jagerman and Honglei Zhuang and Zhen Qin and Xuanhui Wang and Michael Bendersky},
  title = {Query expansion by prompting large language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2305.03653}
}

@article{jeong2024adaptiverag,
  author = {Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung~Ju Hwang and Jong~C Park},
  title = {Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.14403}
}

@article{jin2024ragcache,
  author = {Chao Jin and Zili Zhang and Xuanlin Jiang and Fangyue Liu and Xin Liu and Xuanzhe Liu and Xin Jin},
  title = {Ragcache: Efficient knowledge caching for retrieval-augmented generation},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.12457}
}

@article{khattab2023dspy,
  author = {Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan and Saiful Haq and Ashutosh Sharma and Thomas~T Joshi and Hanna Moazam and et~al.},
  title = {Dspy: Compiling declarative language model calls into self-improving pipelines},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2310.03714}
}

@article{kim2023llmcompiler,
  author = {Sehoon Kim and Suhong Moon and Ryan Tabrizi and Nicholas Lee and Michael~W Mahoney and Kurt Keutzer and Amir Gholami},
  title = {An llm compiler for parallel function calling},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.04511}
}

@inproceedings{kwon2023vllm,
  author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody~Hao Yu and Joseph Gonzalez and Hao Zhang and Ion Stoica},
  title = {Efficient memory management for large language model serving with pagedattention},
  booktitle = {Proc.~ACM SOSP},
  year = {2023}
}

@article{lewis2020retrieval,
  author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{\"u}ttler and Mike Lewis and Wen-tau Yih and Tim Rockt{\"a}schel and et~al.},
  title = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  journal = {arXiv},
  year = {2020}
}

@inproceedings{li2023alpaserve,
  author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph~E Gonzalez and et~al.},
  title = {AlpaServe: Statistical multiplexing with model parallelism for deep learning serving},
  booktitle = {Proc.~USENIX OSDI},
  year = {2023}
}

@article{lin2024infinite,
  author = {Bin Lin and Tao Peng and Chen Zhang and Minmin Sun and Lanbo Li and Hanyu Zhao and Wencong Xiao and Qi~Xu and Xiafei Qiu and Shen Li and et~al.},
  title = {Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.02669}
}

@inproceedings{lin2024parrot,
  author = {Chaofan Lin and Zhenhua Han and Chengruidong Zhang and Yuqing Yang and Fan Yang and Chen Chen and Lili Qiu},
  title = {Parrot: Efficient serving of llm-based applications with semantic variable},
  booktitle = {Proc.~USENIX OSDI},
  year = {2024}
}

@article{lin2021truthfulqa,
  author = {Stephanie Lin and Jacob Hilton and Owain Evans},
  title = {Truthfulqa: Measuring how models mimic human falsehoods},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2109.07958}
}

@article{liu2024optimizingquery,
  author = {Shu Liu and Asim Biswal and Audrey Cheng and Xiangxi Mo and Shiyi Cao and Joseph~E Gonzalez and Ion Stoica and Matei Zaharia},
  title = {Optimizing llm queries in relational workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05821}
}

@article{liu2024optimrelation,
  author = {Shu Liu and Asim Biswal and Audrey Cheng and Xiangxi Mo and Shiyi Cao and Joseph~E Gonzalez and Ion Stoica and Matei Zaharia},
  title = {Optimizing llm queries in relational workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05821}
}

@article{liu2023onlinespec,
  author = {Xiaoxuan Liu and Lanxiang Hu and Peter Bailis and Ion Stoica and Zhijie Deng and Alvin Cheung and Hao Zhang},
  title = {Online speculative decoding},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2310.07177}
}

@article{liu2024iterativere,
  author = {Yanming Liu and Xinyue Peng and Xuhong Zhang and Weihao Liu and Jianwei Yin and Jiannan Cao and Tianyu Du},
  title = {Ra-isf: Learning to answer and understand from retrieval augmentation via iterative self-feedback},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.06840}
}

@inproceedings{madaan2024selfreflection,
  author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and et~al.},
  title = {Self-refine: Iterative refinement with self-feedback},
  booktitle = {Proc.~NeurIPS},
  year = {2024}
}

@inproceedings{miao2024specinfer,
  author = {Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Zhengxin Zhang and Rae Ying~Yee Wong and Alan Zhu and Lijie Yang and Xiaoxiang Shi and et~al.},
  title = {Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  booktitle = {Proc.~ACM ASPLOS},
  year = {2024}
}

@inproceedings{miao2023spotserve,
  author = {Xupeng Miao and Chunan Shi and Jiangfei Duan and Xiaoli Xi and Dahua Lin and Bin Cui and Zhihao Jia},
  title = {Spotserve: Serving generative large language models on preemptible instances},
  booktitle = {Proc.~ACM ASPLOS},
  year = {2024}
}

@inproceedings{moritz2018ray,
  author = {Philipp Moritz and Robert Nishihara and Stephanie Wang and Alexey Tumanov and Richard Liaw and Eric Liang and Melih Elibol and Zongheng Yang and William Paul and Michael~I Jordan and et~al.},
  title = {Ray: A distributed framework for emerging AI applications},
  booktitle = {Proc.~USENIX OSDI},
  year = {2018}
}

@article{ou2024losslessdecoding,
  author = {Jie Ou and Yueming Chen and Wenhong Tian},
  title = {Lossless acceleration of large language model via adaptive n-gram parallel decoding},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.08698}
}

@article{patel2023splitwise,
  author = {Pratyush Patel and Esha Choukse and Chaojie Zhang and {\'I}{\~n}igo Goiri and Aashaka Shah and Saeed Maleki and Ricardo Bianchini},
  title = {Splitwise: Efficient generative llm inference using phase splitting},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.18677}
}

@inproceedings{shen2023hugginggpt,
  author = {Yongliang Shen and Kaitao Song and Xu~Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
  title = {Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  booktitle = {Proc.~NeurIPS},
  year = {2023}
}

@article{sheng2023fairness,
  author = {Ying Sheng and Shiyi Cao and Dacheng Li and Banghua Zhu and Zhuohan Li and Danyang Zhuo and Joseph~E Gonzalez and Ion Stoica},
  title = {Fairness in serving large language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2401.00588}
}

@article{tan2024small,
  author = {Jiejun Tan and Zhicheng Dou and Yutao Zhu and Peidong Guo and Kun Fang and Ji-Rong Wen},
  title = {Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2402.12052}
}

@article{team2024gemma,
  author = {Gemma Team and Morgane Riviere and Shreya Pathak and Pier~Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and L{\'e}onard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ram{\'e} and et~al.},
  title = {Gemma 2: Improving open language models at a practical size},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2408.00118}
}

@article{touvron2023llamamodel,
  author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and et~al.},
  title = {Llama: Open and efficient foundation language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2302.13971}
}

@article{touvron2023llama2model,
  author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and et~al.},
  title = {Llama 2: Open foundation and fine-tuned chat models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2307.09288}
}

@inproceedings{attention,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan~N Gomez and \L~ukasz Kaiser and Illia Polosukhin},
  title = {Attention is All you Need},
  booktitle = {Proc.~NeurIPS},
  year = {2017}
}

@article{wu2024loongserve,
  author = {Bingyang Wu and Shengyu Liu and Yinmin Zhong and Peng Sun and Xuanzhe Liu and Xin Jin},
  title = {Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.09526}
}

@article{wu2023autogen,
  author = {Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li~Jiang and Xiaoyun Zhang and Chi Wang},
  title = {Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.08155}
}

@inproceedings{xiao2023smoothquant,
  author = {Guangxuan Xiao and Ji~Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
  title = {Smoothquant: Accurate and efficient post-training quantization for large language models},
  booktitle = {Proc.~ICML},
  year = {2023}
}

@article{xiao2023bgeembedding,
  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighof},
  title = {C-pack: Packaged resources to advance general chinese embedding},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2309.07597}
}

@inproceedings{yang2018hotpotqa,
  author = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William~W. Cohen and Ruslan Salakhutdinov and Christopher~D. Manning},
  title = {HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  booktitle = {Proc.~EMNLP},
  year = {2018}
}

@inproceedings{yu2022orca,
  author = {Gyeong-In Yu and Joo~Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
  title = {Orca: A distributed serving system for Transformer-Based generative models},
  booktitle = {Proc.~USENIX OSDI},
  year = {2022}
}

@article{zaharia2016apachespark,
  author = {Matei Zaharia and Reynold~S Xin and Patrick Wendell and Tathagata Das and Michael Armbrust and Ankur Dave and Xiangrui Meng and Josh Rosen and Shivaram Venkataraman and Michael~J Franklin and et~al.},
  title = {Apache spark: a unified engine for big data processing},
  journal = {Communications of the ACM},
  year = {2016}
}

@inproceedings{zhang2023shepherd,
  author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
  title = {SHEPHERD: Serving DNNs in the wild},
  booktitle = {Proc.~USENIX NSDI},
  year = {2023}
}

@article{zheng2023sglang,
  author = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody~Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph~E Gonzalez and et~al.},
  title = {Efficiently programming large language models using sglang},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.07104}
}

@article{zhong2024distserve,
  author = {Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
  title = {Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.09670}
}

@article{zhu2023optimalcache,
  author = {Banghua Zhu and Ying Sheng and Lianmin Zheng and Clark Barrett and Michael~I Jordan and Jiantao Jiao},
  title = {On optimal caching and model multiplexing for large model inference},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2306.02003}
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}
@article{shah2024flashattention,
  title={FlashAttention-3: Fast and Accurate Attention with Asynchrony and Low-precision},
  author={Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
  journal={arXiv preprint arXiv:2407.08608},
  year={2024}
}
@misc{tensorrtllm, title={TensorRT-LLM: A TensorRT Toolbox for Optimized Large Language Model Inference}, url={https://github.com/NVIDIA/TensorRT-LLM}, journal={Nvidia}}
@article{liu2024andes,
  title={Andes: Defining and Enhancing Quality-of-Experience in LLM-Based Text Streaming Services},
  author={Liu, Jiachen and Wu, Zhiyu and Chung, Jae-Won and Lai, Fan and Lee, Myungjin and Chowdhury, Mosharaf},
  journal={arXiv preprint arXiv:2404.16283},
  year={2024}
}
@misc{tgi, title={Text Generation Inference: A Rust, Python and gRPC server for text generation inference.}, url={https://github.com/huggingface/text-generation-inference}, journal={HuggingFace}}

@article{zheng2024response,
  title={Response length perception and sequence scheduling: An llm-empowered llm inference pipeline},
  author={Zheng, Zangwei and Ren, Xiaozhe and Xue, Fuzhao and Luo, Yang and Jiang, Xin and You, Yang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{hong2024flashdecoding++,
  title={FlashDecoding++: Faster Large Language Model Inference with Asynchronization, Flat GEMM Optimization, and Heuristics},
  author={Hong, Ke and Dai, Guohao and Xu, Jiaming and Mao, Qiuli and Li, Xiuhong and Liu, Jun and Dong, Yuhan and Wang, Yu and others},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={148--161},
  year={2024}
}
@inproceedings {298503,
author = {Kinman Lei and Yuyang Jin and Mingshu Zhai and Kezhao Huang and Haoxing Ye and Jidong Zhai},
title = {{PUZZLE}: Efficiently Aligning Large Language Models through {Light-Weight} Context Switch},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {127--140},
url = {https://www.usenix.org/conference/atc24/presentation/lei},
publisher = {USENIX Association},
month = jul
}
@article{sheng2023fairness,
  title={Fairness in serving large language models},
  author={Sheng, Ying and Cao, Shiyi and Li, Dacheng and Zhu, Banghua and Li, Zhuohan and Zhuo, Danyang and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2401.00588},
  year={2023}
}
@article{li2024toward,
  title={Toward sustainable genai using generation directives for carbon-friendly large language model inference},
  author={Li, Baolin and Jiang, Yankai and Gadepally, Vijay and Tiwari, Devesh},
  journal={arXiv preprint arXiv:2403.12900},
  year={2024}
}
@article{huang2023towards,
  title={Towards MoE Deployment: Mitigating Inefficiencies in Mixture-of-Expert (MoE) Inference},
  author={Huang, Haiyang and Ardalani, Newsha and Sun, Anna and Ke, Liu and Lee, Hsien-Hsin S and Sridhar, Anjali and Bhosale, Shruti and Wu, Carole-Jean and Lee, Benjamin},
  journal={arXiv preprint arXiv:2303.06182},
  year={2023}
}
@article{kamahori2024fiddler,
  title={Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models},
  author={Kamahori, Keisuke and Gu, Yile and Zhu, Kan and Kasikci, Baris},
  journal={arXiv preprint arXiv:2402.07033},
  year={2024}
}
@article{xue2024moe,
  title={Moe-infinity: Activation-aware expert offloading for efficient moe serving},
  author={Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh},
  journal={arXiv preprint arXiv:2401.14361},
  year={2024}
}
@article{du2024sida,
  title={SiDA: Sparsity-Inspired Data-Aware Serving for Efficient and Scalable Large Mixture-of-Experts Models},
  author={Du, Zhixu and Li, Shiyu and Wu, Yuhao and Jiang, Xiangyu and Sun, Jingwei and Zheng, Qilin and Wu, Yongkai and Li, Ang and Li, Hai and Chen, Yiran},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={224--238},
  year={2024}
}
@inproceedings{yao2024exploiting,
  title={Exploiting Inter-Layer Expert Affinity for Accelerating Mixture-of-Experts Model Inference},
  author={Yao, Jinghan and Anthony, Quentin and Shafi, Aamir and Subramoni, Hari and Panda, Dhabaleswar K DK},
  booktitle={2024 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
  pages={915--925},
  year={2024},
  organization={IEEE}
}
@inproceedings{li2023accelerating,
  title={Accelerating distributed $\{$MoE$\}$ training and inference with lina},
  author={Li, Jiamin and Jiang, Yimin and Zhu, Yibo and Wang, Cong and Xu, Hong},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={945--959},
  year={2023}
}
@article{xuanlei2024hetegen,
  title={HeteGen: Efficient Heterogeneous Parallel Inference for Large Language Models on Resource-Constrained Devices},
  author={XUANLEI, ZHAO and Jia, Bin and Zhou, Haotian and Liu, Ziming and Cheng, Shenggan and You, Yang},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={162--172},
  year={2024}
}
@article{miao2024flexllm,
  title={FlexLLM: A System for Co-Serving Large Language Model Inference and Parameter-Efficient Finetuning},
  author={Miao, Xupeng and Oliaro, Gabriele and Cheng, Xinhao and Wu, Mengdi and Unger, Colin and Jia, Zhihao},
  journal={arXiv preprint arXiv:2402.18789},
  year={2024}
}
@article{yang2024perllm,
  title={PerLLM: Personalized Inference Scheduling with Edge-Cloud Collaboration for Diverse LLM Services},
  author={Yang, Zheming and Yang, Yuanhao and Zhao, Chang and Guo, Qi and He, Wenkai and Ji, Wen},
  journal={arXiv preprint arXiv:2405.14636},
  year={2024}
}
@inproceedings{patel2024characterizing,
  title={Characterizing Power Management Opportunities for LLMs in the Cloud},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Warrier, Brijesh and Mahalingam, Nithish and Bianchini, Ricardo},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={207--222},
  year={2024}
}
@article{fu2024serverlessllm,
  title={ServerlessLLM: Locality-Enhanced Serverless Inference for Large Language Models},
  author={Fu, Yao and Xue, Leyang and Huang, Yeqi and Brabete, Andrei-Octavian and Ustiugov, Dmitrii and Patel, Yuvraj and Mai, Luo},
  journal={arXiv preprint arXiv:2401.14351},
  year={2024}
}
@article{sun2024llumnix,
  title={Llumnix: Dynamic Scheduling for Large Language Model Serving},
  author={Sun, Biao and Huang, Ziming and Zhao, Hanyu and Xiao, Wencong and Zhang, Xinyi and Li, Yong and Lin, Wei},
  journal={arXiv preprint arXiv:2406.03243},
  year={2024}
}
@article{griggs2024m,
  title={M$\backslash$'elange: Cost Efficient Large Language Model Serving by Exploiting GPU Heterogeneity},
  author={Griggs, Tyler and Liu, Xiaoxuan and Yu, Jiaxiang and Kim, Doyoung and Chiang, Wei-Lin and Cheung, Alvin and Stoica, Ion},
  journal={arXiv preprint arXiv:2404.14527},
  year={2024}
}
@inproceedings{miao2024spotserve,
  title={Spotserve: Serving generative large language models on preemptible instances},
  author={Miao, Xupeng and Shi, Chunan and Duan, Jiangfei and Xi, Xiaoli and Lin, Dahua and Cui, Bin and Jia, Zhihao},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={1112--1127},
  year={2024}
}
@article{mei2024helix,
  title={Helix: Distributed Serving of Large Language Models via Max-Flow on Heterogeneous GPUs},
  author={Mei, Yixuan and Zhuang, Yonghao and Miao, Xupeng and Yang, Juncheng and Jia, Zhihao and Vinayak, Rashmi},
  journal={arXiv preprint arXiv:2406.01566},
  year={2024}
}
@article{zhong2024distserve,
  title={Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  author={Zhong, Yinmin and Liu, Shengyu and Chen, Junda and Hu, Jianbo and Zhu, Yibo and Liu, Xuanzhe and Jin, Xin and Zhang, Hao},
  journal={arXiv preprint arXiv:2401.09670},
  year={2024}
}
@inproceedings{oh2024exegpt,
  title={Exegpt: Constraint-aware resource scheduling for llm inference},
  author={Oh, Hyungjun and Kim, Kihong and Kim, Jaemin and Kim, Sungkyun and Lee, Junyeol and Chang, Du-seong and Seo, Jiwon},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={369--384},
  year={2024}
}
@article{patel2023splitwise,
  title={Splitwise: Efficient generative llm inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Goiri, {\'I}{\~n}igo and Shah, Aashaka and Maleki, Saeed and Bianchini, Ricardo},
  journal={arXiv preprint arXiv:2311.18677},
  year={2023}
}
@article{hu2024inference,
  title={Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  author={Hu, Cunchen and Huang, Heyang and Xu, Liangliang and Chen, Xusheng and Xu, Jiang and Chen, Shuang and Feng, Hao and Wang, Chenxi and Wang, Sa and Bao, Yungang and others},
  journal={arXiv preprint arXiv:2401.11181},
  year={2024}
}

@article{agrawal2024taming,
  title={Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  author={Agrawal, Amey and Kedia, Nitin and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Tumanov, Alexey and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2403.02310},
  year={2024}
}
@article{holmes2024deepspeed,
  title={Deepspeed-fastgen: High-throughput text generation for llms via mii and deepspeed-inference},
  author={Holmes, Connor and Tanaka, Masahiro and Wyatt, Michael and Awan, Ammar Ahmad and Rasley, Jeff and Rajbhandari, Samyam and Aminabadi, Reza Yazdani and Qin, Heyang and Bakhtiari, Arash and Kurilenko, Lev and others},
  journal={arXiv preprint arXiv:2401.08671},
  year={2024}
}
@article{jin2023s,
  title={S3: Increasing GPU Utilization during Generative Inference for Higher Throughput},
  author={Jin, Yunho and Wu, Chun-Feng and Brooks, David and Wei, Gu-Yeon},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={18015--18027},
  year={2023}
}

@article{prabhu2024vattention,
  title={vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention},
  author={Prabhu, Ramya and Nayak, Ajay and Mohan, Jayashree and Ramjee, Ramachandran and Panwar, Ashish},
  journal={arXiv preprint arXiv:2405.04437},
  year={2024}
}

@article{gim2024prompt,
  title={Prompt cache: Modular attention reuse for low-latency inference},
  author={Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={325--338},
  year={2024}
}

@inproceedings {gaocost,
author = {Bin Gao and Zhuomin He and Puru Sharma and Qingxuan Kang and Djordje Jevdjic and Junbo Deng and Xingkun Yang and Zhou Yu and Pengfei Zuo},
title = {{Cost-Efficient} Large Language Model Serving for Multi-turn Conversations with {CachedAttention}},
booktitle = {2024 USENIX Annual Technical Conference (USENIX ATC 24)},
year = {2024},
isbn = {978-1-939133-41-0},
address = {Santa Clara, CA},
pages = {111--126},
url = {https://www.usenix.org/conference/atc24/presentation/gao-bin-cost},
publisher = {USENIX Association},
month = jul
}

@inproceedings{yu2022orca,
  title={Orca: A distributed serving system for $\{$Transformer-Based$\}$ generative models},
  author={Yu, Gyeong-In and Jeong, Joo Seong and Kim, Geon-Woo and Kim, Soojeong and Chun, Byung-Gon},
  booktitle={16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},
  pages={521--538},
  year={2022}
}
@article{liu2023ring,
  title={Ring attention with blockwise transformers for near-infinite context},
  author={Liu, Hao and Zaharia, Matei and Abbeel, Pieter},
  journal={arXiv preprint arXiv:2310.01889},
  year={2023}
}
@article{kang2024gear,
  title={Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm},
  author={Kang, Hao and Zhang, Qingru and Kundu, Souvik and Jeong, Geonhwa and Liu, Zaoxing and Krishna, Tushar and Zhao, Tuo},
  journal={arXiv preprint arXiv:2403.05527},
  year={2024}
}
@article{liu2024kivi,
  title={Kivi: A tuning-free asymmetric 2bit quantization for kv cache},
  author={Liu, Zirui and Yuan, Jiayi and Jin, Hongye and Zhong, Shaochen and Xu, Zhaozhuo and Braverman, Vladimir and Chen, Beidi and Hu, Xia},
  journal={arXiv preprint arXiv:2402.02750},
  year={2024}
}
@article{liu2024minicache,
  title={MiniCache: KV Cache Compression in Depth Dimension for Large Language Models},
  author={Liu, Akide and Liu, Jing and Pan, Zizheng and He, Yefei and Haffari, Gholamreza and Zhuang, Bohan},
  journal={arXiv preprint arXiv:2405.14366},
  year={2024}
}
@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{lin2024infinite,
  title={Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
  author={Lin, Bin and Peng, Tao and Zhang, Chen and Sun, Minmin and Li, Lanbo and Zhao, Hanyu and Xiao, Wencong and Xu, Qi and Qiu, Xiafei and Li, Shen and others},
  journal={arXiv preprint arXiv:2401.02669},
  year={2024}
}
@article{hu2024memserve,
  title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool},
  author={Hu, Cunchen and Huang, Heyang and Hu, Junhao and Xu, Jiang and Chen, Xusheng and Xie, Tao and Wang, Chenxi and Wang, Sa and Bao, Yungang and Sun, Ninghui and others},
  journal={arXiv preprint arXiv:2406.17565},
  year={2024}
}
@article{lee2024infinigen,
  title={InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management},
  author={Lee, Wonbeom and Lee, Jungi and Seo, Junghwan and Sim, Jaewoong},
  journal={arXiv preprint arXiv:2406.19707},
  year={2024}
}

@article{wu2024loongserve,
  title={LoongServe: Efficiently Serving Long-context Large Language Models with Elastic Sequence Parallelism},
  author={Wu, Bingyang and Liu, Shengyu and Zhong, Yinmin and Sun, Peng and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.09526},
  year={2024}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA}
}

@article{pope2023efficiently,
  title={Efficiently scaling transformer inference},
  author={Pope, Reiner and Douglas, Sholto and Chowdhery, Aakanksha and Devlin, Jacob and Bradbury, James and Heek, Jonathan and Xiao, Kefan and Agrawal, Shivani and Dean, Jeff},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={606--624},
  year={2023}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}
@article{yuan2024llm,
  title={Llm inference unveiled: Survey and roofline model insights},
  author={Yuan, Zhihang and Shang, Yuzhang and Zhou, Yang and Dong, Zhen and Xue, Chenhao and Wu, Bingzhe and Li, Zhikai and Gu, Qingyi and Lee, Yong Jae and Yan, Yan and others},
  journal={arXiv preprint arXiv:2402.16363},
  year={2024}
}
@article{miao2023towards,
  title={Towards efficient generative large language model serving: A survey from algorithms to systems},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  journal={arXiv preprint arXiv:2312.15234},
  year={2023}
}
@article{zhou2024survey,
  title={A survey on efficient inference for large language models},
  author={Zhou, Zixuan and Ning, Xuefei and Hong, Ke and Fu, Tianyu and Xu, Jiaming and Li, Shiyao and Lou, Yuming and Wang, Luning and Yuan, Zhihang and Li, Xiuhong and others},
  journal={arXiv preprint arXiv:2404.14294},
  year={2024}
}
@article{adnan2024keyformer,
  title={Keyformer: Kv cache reduction through key tokens selection for efficient generative inference},
  author={Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
  journal={Proceedings of Machine Learning and Systems},
  volume={6},
  pages={114--127},
  year={2024}
}
@article{fu2024break,
  title={Break the sequential dependency of llm inference using lookahead decoding},
  author={Fu, Yichao and Bailis, Peter and Stoica, Ion and Zhang, Hao},
  journal={arXiv preprint arXiv:2402.02057},
  year={2024}
}
@article{cai2024medusa,
  title={Medusa: Simple llm inference acceleration framework with multiple decoding heads},
  author={Cai, Tianle and Li, Yuhong and Geng, Zhengyang and Peng, Hongwu and Lee, Jason D and Chen, Deming and Dao, Tri},
  journal={arXiv preprint arXiv:2401.10774},
  year={2024}
}
@article{yao2024cacheblend,
  title={CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion},
  author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
  journal={arXiv preprint arXiv:2405.16444},
  year={2024}
}
@article{jin2024ragcache,
  title={RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation},
  author={Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
  journal={arXiv preprint arXiv:2404.12457},
  year={2024}
}
@article{zhu2024accelerating,
  title={Accelerating Inference of Retrieval-Augmented Generation via Sparse Context Selection},
  author={Zhu, Yun and Gu, Jia-Chen and Sikora, Caitlin and Ko, Ho and Liu, Yinxiao and Lin, Chu-Cheng and Shu, Lei and Luo, Liangchen and Meng, Lei and Liu, Bang and others},
  journal={arXiv preprint arXiv:2405.16178},
  year={2024}
}
@article{ong2024routellm,
  title={RouteLLM: Learning to Route LLMs with Preference Data},
  author={Ong, Isaac and Almahairi, Amjad and Wu, Vincent and Chiang, Wei-Lin and Wu, Tianhao and Gonzalez, Joseph E and Kadous, M Waleed and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.18665},
  year={2024}
}

@inproceedings{miao2024specinfer,
  title={Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Wang, Zeyu and Zhang, Zhengxin and Wong, Rae Ying Yee and Zhu, Alan and Yang, Lijie and Shi, Xiaoxiang and others},
  booktitle={Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
  pages={932--949},
  year={2024}
}

@article{lin2024parrot,
  title={Parrot: Efficient Serving of LLM-based Applications with Semantic Variable},
  author={Lin, Chaofan and Han, Zhenhua and Zhang, Chengruidong and Yang, Yuqing and Yang, Fan and Chen, Chen and Qiu, Lili},
  journal={arXiv preprint arXiv:2405.19888},
  year={2024}
}

@article{chen2023frugalgpt,
  title={Frugalgpt: How to use large language models while reducing cost and improving performance},
  author={Chen, Lingjiao and Zaharia, Matei and Zou, James},
  journal={arXiv preprint arXiv:2305.05176},
  year={2023}
}








































