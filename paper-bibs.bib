@article{tan2024teola,
  title={Teola: Towards end-to-end optimization of llm-based applications},
  author={Tan, Xin and Jiang, Yimin and Yang, Yitao and Xu, Hong},
  journal={arXiv preprint arXiv:2407.00326},
  year={2024}
}

Here are the transformed .bib formats for the provided papers, along with the requested paper formats:

```bibtex
@misc{llamaindex,
  title = {LlamaIndex},
  year = {2022},
  url = {https://github.com/jerryjliu/llama_index}
}

@misc{promptflow,
  title = {Promptflow},
  year = {2023},
  url = {https://https://github.com/microsoft/promptflow}
}

@misc{autogpt,
  title = {Autogpt},
  year = {2024},
  url = {https://github.com/Significant-Gravitas/AutoGPT}
}

@misc{bing,
  title = {Bing Copilot},
  year = {2024},
  url = {https://www.bing.com/chat}
}

@misc{characterai,
  title = {Character.ai},
  year = {2024},
  url = {https://character.ai/}
}

@misc{contextual-retrieval,
  title = {contextual-retrieval},
  year = {2024},
  url = {https://www.anthropic.com/news/contextual-retrieval}
}

@misc{Fastapi,
  title = {Fastapi},
  year = {2024},
  url = {https://fastapi.tiangolo.com/}
}

@misc{Finqabench,
  title = {Finqabench Dataset},
  year = {2024},
  url = {https://huggingface.co/datasets/lighthouzai/finqabench}
}

@misc{googlesearch,
  title = {Google custom search},
  year = {2024},
  url = {https://programmablesearchengine.google.com/}
}

@misc{azurerag,
  title = {Gpt-rag},
  year = {2024},
  url = {https://github.com/Azure/GPT-RAG}
}

@misc{haystack,
  title = {haystack},
  year = {2024},
  url = {https://github.com/deepset-ai/haystack}
}

@misc{langchain,
  title = {Langchain},
  year = {2024},
  url = {https://github.com/langchain-ai/langchain}
}

@misc{langgraph,
  title = {LangGraph},
  year = {2024},
  url = {https://python.langchain.com/docs/langgraph/}
}

@misc{lazyllm,
  title = {Lazyllm},
  year = {2024},
  url = {https://github.com/LazyAGI/LazyLLM}
}

@misc{alibaba_llmapp_observation,
  title = {Observability of llm applications: Exploration and practice from the perspective of trace},
  year = {2024},
  url = {https://www.alibabacloud.com/blog/observability-of-llm-applications-exploration-and-practice-from-the-perspective-of-trace_601604}
}

@misc{openaifunc,
  title = {Openai function calling},
  year = {2024},
  url = {https://platform.openai.com/docs/guides/function-calling}
}

@misc{pairag,
  title = {Pairag},
  year = {2024},
  url = {https://github.com/aigc-apps/PAI-RAG}
}

@misc{perplexity,
  title = {Perplexity ai},
  year = {2024},
  url = {https://www.perplexity.ai/}
}

@misc{pgvector,
  title = {Pgvector},
  year = {2024},
  url = {https://github.com/pgvector/pgvector}
}

@misc{postgresql,
  title = {Postgresql},
  year = {2024},
  url = {https://www.postgresql.org/}
}

@misc{privatellm,
  title = {Privatellm},
  year = {2024},
  url = {https://privatellm.app/en}
}

@misc{tritonserver,
  title = {Triton inference server},
  year = {2024},
  url = {https://github.com/triton-inference-server}
}

@inproceedings{tensorflow,
  author = {Mart{\'\i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek~G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
  title = {TensorFlow: A system for Large-Scale machine learning},
  booktitle = {Proc.~USENIX OSDI},
  year = {2016}
}

@article{agrawal2024taming,
  author = {Amey Agrawal and Nitin Kedia and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav~S Gulavani and Alexey Tumanov and Ramachandran Ramjee},
  title = {Taming throughput-latency tradeoff in llm inference with sarathi-serve},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.02310}
}

@article{agrawal2023sarathi,
  author = {Amey Agrawal and Ashish Panwar and Jayashree Mohan and Nipun Kwatra and Bhargav~S Gulavani and Ramachandran Ramjee},
  title = {Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.16369}
}

@inproceedings{bang2023gptcache,
  author = {Fu~Bang},
  title = {Gptcache: An open-source semantic cache for llm applications enabling faster answers and cost savings},
  booktitle = {Proc.~the 3rd Workshop for Natural Language Processing Open Source Software},
  year = {2023}
}

@inproceedings{bauer2012legion,
  author = {Michael Bauer and Sean Treichler and Elliott Slaughter and Alex Aiken},
  title = {Legion: Expressing locality and independence with logical regions},
  booktitle = {Proc. IEEE SC},
  year = {2012}
}

@inproceedings{webquestion,
  author = {Jonathan Berant and Andrew Chou and Roy Frostig and Percy Liang},
  title = {Semantic parsing on Freebase from question-answer pairs},
  booktitle = {Proc.~EMNLP},
  year = {2013},
  month = {October}
}

@inproceedings{crankshaw2017clipper,
  author = {Daniel Crankshaw and Xin Wang and Guilio Zhou and Michael~J Franklin and Joseph~E Gonzalez and Ion Stoica},
  title = {Clipper: A Low-Latency online prediction serving system},
  booktitle = {Proc.~USENIX NSDI},
  year = {2017}
}

@inproceedings{dao2022flashattention,
  author = {Tri Dao and Dan Fu and Stefano Ermon and Atri Rudra and Christopher R{\'e}},
  title = {Flashattention: Fast and memory-efficient exact attention with io-awareness},
  booktitle = {Proc.~NeurIPS},
  year = {2022}
}

@inproceedings{devlin2018bert,
  author = {Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  title = {Bert: Pre-training of deep bidirectional transformers for language understanding},
  booktitle = {Proc.~ACL},
  year = {2018}
}

@article{gao2023retrievalsurvey,
  author = {Yunfan Gao and Yun Xiong and Xinyu Gao and Kangxiang Jia and Jinliu Pan and Yuxi Bi and Yi~Dai and Jiawei Sun and Haofen Wang},
  title = {Retrieval-augmented generation for large language models: A survey},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.10997}
}

@article{gim2023promptcache,
  author = {In~Gim and Guojun Chen and Seung-seob Lee and Nikhil Sarda and Anurag Khandelwal and Lin Zhong},
  title = {Prompt cache: Modular attention reuse for low-latency inference},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.04934}
}

@inproceedings{gog2015musketeer,
  author = {Ionel Gog and Malte Schwarzkopf and Natacha Crooks and Matthew~P Grosvenor and Allen Clement and Steven Hand},
  title = {Musketeer: all for one, one for all in data processing systems},
  booktitle = {Proc. ACM Eurosys},
  year = {2015}
}

@inproceedings{gujarati2020clockwork,
  author = {Arpan Gujarati and Reza Karimi and Safya Alzayat and Wei Hao and Antoine Kaufmann and Ymir Vigfusson and Jonathan Mace},
  title = {Serving DNNs like clockwork: Performance predictability from the bottom up},
  booktitle = {Proc.~USENIX OSDI},
  year = {2020}
}

@inproceedings{hong2023flashdecoding++,
  author = {Ke~Hong and Guohao Dai and Jiaming Xu and Qiuli Mao and Xiuhong Li and Jun Liu and Kangdi Chen and Hanyu Dong and Yu~Wang},
  title = {Flashdecoding++: Faster large language model inference on gpus},
  booktitle = {Proc.~Machine Learning and Systems},
  year = {2023}
}

@article{hong2024data,
  author = {Sirui Hong and Yizhang Lin and Bang Liu and Bangbang Liu and Binhao Wu and Danyang Li and Jiaqi Chen and Jiayi Zhang and Jinlin Wang and Li~Zhang and Lingyao Zhang and Min Yang and Mingchen Zhuge and Taicheng Guo and Tuo Zhou and Wei Tao and Wenyi Wang and Xiangru Tang and Xiangtao Lu and Xiawu Zheng and Xinbing Liang and Yaying Fei and Yuheng Cheng and Zongze Xu and Chenglin Wu},
  title = {Data interpreter: An llm agent for data science},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2402.18679}
}

@article{hong2023metagpt,
  author = {Sirui Hong and Mingchen Zhuge and Jonathan Chen and Xiawu Zheng and Yuheng Cheng and Ceyao Zhang and Jinlin Wang and Zili Wang and Steven Ka~Shing Yau and Zijuan Lin and Liyang Zhou and Chenyu Ran and Lingfeng Xiao and Chenglin Wu and J{\"u}rgen Schmidhuber},
  title = {Metagpt: Meta programming for a multi-agent collaborative framework},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.00352}
}

@article{hu2024inferencewithoutinterference,
  author = {Cunchen Hu and Heyang Huang and Liangliang Xu and Xusheng Chen and Jiang Xu and Shuang Chen and Hao Feng and Chenxi Wang and Sa~Wang and Yungang Bao and et~al.},
  title = {Inference without interference: Disaggregate llm inference for mixed downstream workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.11181}
}

@article{huang2023Hallucination,
  author = {Lei Huang and Weijiang Yu and Weitao Ma and Weihong Zhong and Zhangyin Feng and Haotian Wang and Qianglong Chen and Weihua Peng and Xiaocheng Feng and Bing Qin and Ting Liu},
  title = {A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.05232}
}

@article{huang2024tool,
  author = {Zhongzhen Huang and Kui Xue and Yongqi Fan and Linjie Mu and Ruoyu Liu and Tong Ruan and Shaoting Zhang and Xiaofan Zhang},
  title = {Tool calling: Enhancing medication consultation via retrieval-augmented large language models},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.17897}
}

@inproceedings{isard2007dryad,
  author = {Michael Isard and Mihai Budiu and Yuan Yu and Andrew Birrell and Dennis Fetterly},
  title = {Dryad: distributed data-parallel programs from sequential building blocks},
  booktitle = {Proc.~ACM Eurosys},
  year = {2007}
}

@article{jagerman2023queryexpansion,
  author = {Rolf Jagerman and Honglei Zhuang and Zhen Qin and Xuanhui Wang and Michael Bendersky},
  title = {Query expansion by prompting large language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2305.03653}
}

@article{jeong2024adaptiverag,
  author = {Soyeong Jeong and Jinheon Baek and Sukmin Cho and Sung~Ju Hwang and Jong~C Park},
  title = {Adaptive-rag: Learning to adapt retrieval-augmented large language models through question complexity},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.14403}
}

@article{jin2024ragcache,
  author = {Chao Jin and Zili Zhang and Xuanlin Jiang and Fangyue Liu and Xin Liu and Xuanzhe Liu and Xin Jin},
  title = {Ragcache: Efficient knowledge caching for retrieval-augmented generation},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.12457}
}

@article{khattab2023dspy,
  author = {Omar Khattab and Arnav Singhvi and Paridhi Maheshwari and Zhiyuan Zhang and Keshav Santhanam and Sri Vardhamanan and Saiful Haq and Ashutosh Sharma and Thomas~T Joshi and Hanna Moazam and et~al.},
  title = {Dspy: Compiling declarative language model calls into self-improving pipelines},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2310.03714}
}

@article{kim2023llmcompiler,
  author = {Sehoon Kim and Suhong Moon and Ryan Tabrizi and Nicholas Lee and Michael~W Mahoney and Kurt Keutzer and Amir Gholami},
  title = {An llm compiler for parallel function calling},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.04511}
}

@inproceedings{kwon2023vllm,
  author = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody~Hao Yu and Joseph Gonzalez and Hao Zhang and Ion Stoica},
  title = {Efficient memory management for large language model serving with pagedattention},
  booktitle = {Proc.~ACM SOSP},
  year = {2023}
}

@article{lewis2020retrieval,
  author = {Patrick Lewis and Ethan Perez and Aleksandra Piktus and Fabio Petroni and Vladimir Karpukhin and Naman Goyal and Heinrich K{\"u}ttler and Mike Lewis and Wen-tau Yih and Tim Rockt{\"a}schel and et~al.},
  title = {Retrieval-augmented generation for knowledge-intensive nlp tasks},
  journal = {arXiv},
  year = {2020}
}

@inproceedings{li2023alpaserve,
  author = {Zhuohan Li and Lianmin Zheng and Yinmin Zhong and Vincent Liu and Ying Sheng and Xin Jin and Yanping Huang and Zhifeng Chen and Hao Zhang and Joseph~E Gonzalez and et~al.},
  title = {AlpaServe: Statistical multiplexing with model parallelism for deep learning serving},
  booktitle = {Proc.~USENIX OSDI},
  year = {2023}
}

@article{lin2024infinite,
  author = {Bin Lin and Tao Peng and Chen Zhang and Minmin Sun and Lanbo Li and Hanyu Zhao and Wencong Xiao and Qi~Xu and Xiafei Qiu and Shen Li and et~al.},
  title = {Infinite-llm: Efficient llm service for long context with distattention and distributed kvcache},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.02669}
}

@inproceedings{lin2024parrot,
  author = {Chaofan Lin and Zhenhua Han and Chengruidong Zhang and Yuqing Yang and Fan Yang and Chen Chen and Lili Qiu},
  title = {Parrot: Efficient serving of llm-based applications with semantic variable},
  booktitle = {Proc.~USENIX OSDI},
  year = {2024}
}

@article{lin2021truthfulqa,
  author = {Stephanie Lin and Jacob Hilton and Owain Evans},
  title = {Truthfulqa: Measuring how models mimic human falsehoods},
  journal = {arXiv},
  year = {2021},
  url = {https://arxiv.org/abs/2109.07958}
}

@article{liu2024optimizingquery,
  author = {Shu Liu and Asim Biswal and Audrey Cheng and Xiangxi Mo and Shiyi Cao and Joseph~E Gonzalez and Ion Stoica and Matei Zaharia},
  title = {Optimizing llm queries in relational workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05821}
}

@article{liu2024optimrelation,
  author = {Shu Liu and Asim Biswal and Audrey Cheng and Xiangxi Mo and Shiyi Cao and Joseph~E Gonzalez and Ion Stoica and Matei Zaharia},
  title = {Optimizing llm queries in relational workloads},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.05821}
}

@article{liu2023onlinespec,
  author = {Xiaoxuan Liu and Lanxiang Hu and Peter Bailis and Ion Stoica and Zhijie Deng and Alvin Cheung and Hao Zhang},
  title = {Online speculative decoding},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2310.07177}
}

@article{liu2024iterativere,
  author = {Yanming Liu and Xinyue Peng and Xuhong Zhang and Weihao Liu and Jianwei Yin and Jiannan Cao and Tianyu Du},
  title = {Ra-isf: Learning to answer and understand from retrieval augmentation via iterative self-feedback},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2403.06840}
}

@inproceedings{madaan2024selfreflection,
  author = {Aman Madaan and Niket Tandon and Prakhar Gupta and Skyler Hallinan and Luyu Gao and Sarah Wiegreffe and Uri Alon and Nouha Dziri and Shrimai Prabhumoye and Yiming Yang and et~al.},
  title = {Self-refine: Iterative refinement with self-feedback},
  booktitle = {Proc.~NeurIPS},
  year = {2024}
}

@inproceedings{miao2024specinfer,
  author = {Xupeng Miao and Gabriele Oliaro and Zhihao Zhang and Xinhao Cheng and Zeyu Wang and Zhengxin Zhang and Rae Ying~Yee Wong and Alan Zhu and Lijie Yang and Xiaoxiang Shi and et~al.},
  title = {Specinfer: Accelerating large language model serving with tree-based speculative inference and verification},
  booktitle = {Proc.~ACM ASPLOS},
  year = {2024}
}

@inproceedings{miao2023spotserve,
  author = {Xupeng Miao and Chunan Shi and Jiangfei Duan and Xiaoli Xi and Dahua Lin and Bin Cui and Zhihao Jia},
  title = {Spotserve: Serving generative large language models on preemptible instances},
  booktitle = {Proc.~ACM ASPLOS},
  year = {2024}
}

@inproceedings{moritz2018ray,
  author = {Philipp Moritz and Robert Nishihara and Stephanie Wang and Alexey Tumanov and Richard Liaw and Eric Liang and Melih Elibol and Zongheng Yang and William Paul and Michael~I Jordan and et~al.},
  title = {Ray: A distributed framework for emerging AI applications},
  booktitle = {Proc.~USENIX OSDI},
  year = {2018}
}

@article{ou2024losslessdecoding,
  author = {Jie Ou and Yueming Chen and Wenhong Tian},
  title = {Lossless acceleration of large language model via adaptive n-gram parallel decoding},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.08698}
}

@article{patel2023splitwise,
  author = {Pratyush Patel and Esha Choukse and Chaojie Zhang and {\'I}{\~n}igo Goiri and Aashaka Shah and Saeed Maleki and Ricardo Bianchini},
  title = {Splitwise: Efficient generative llm inference using phase splitting},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2311.18677}
}

@inproceedings{shen2023hugginggpt,
  author = {Yongliang Shen and Kaitao Song and Xu~Tan and Dongsheng Li and Weiming Lu and Yueting Zhuang},
  title = {Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
  booktitle = {Proc.~NeurIPS},
  year = {2023}
}

@article{sheng2023fairness,
  author = {Ying Sheng and Shiyi Cao and Dacheng Li and Banghua Zhu and Zhuohan Li and Danyang Zhuo and Joseph~E Gonzalez and Ion Stoica},
  title = {Fairness in serving large language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2401.00588}
}

@article{tan2024small,
  author = {Jiejun Tan and Zhicheng Dou and Yutao Zhu and Peidong Guo and Kun Fang and Ji-Rong Wen},
  title = {Small models, big insights: Leveraging slim proxy models to decide when and what to retrieve for llms},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2402.12052}
}

@article{team2024gemma,
  author = {Gemma Team and Morgane Riviere and Shreya Pathak and Pier~Giuseppe Sessa and Cassidy Hardin and Surya Bhupatiraju and L{\'e}onard Hussenot and Thomas Mesnard and Bobak Shahriari and Alexandre Ram{\'e} and et~al.},
  title = {Gemma 2: Improving open language models at a practical size},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2408.00118}
}

@article{touvron2023llamamodel,
  author = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timoth{\'e}e Lacroix and Baptiste Rozi{\`e}re and Naman Goyal and Eric Hambro and Faisal Azhar and et~al.},
  title = {Llama: Open and efficient foundation language models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2302.13971}
}

@article{touvron2023llama2model,
  author = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and et~al.},
  title = {Llama 2: Open foundation and fine-tuned chat models},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2307.09288}
}

@inproceedings{attention,
  author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan~N Gomez and \L~ukasz Kaiser and Illia Polosukhin},
  title = {Attention is All you Need},
  booktitle = {Proc.~NeurIPS},
  year = {2017}
}

@article{wu2024loongserve,
  author = {Bingyang Wu and Shengyu Liu and Yinmin Zhong and Peng Sun and Xuanzhe Liu and Xin Jin},
  title = {Loongserve: Efficiently serving long-context large language models with elastic sequence parallelism},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2404.09526}
}

@article{wu2023autogen,
  author = {Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Shaokun Zhang and Erkang Zhu and Beibin Li and Li~Jiang and Xiaoyun Zhang and Chi Wang},
  title = {Autogen: Enabling next-gen llm applications via multi-agent conversation framework},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2308.08155}
}

@inproceedings{xiao2023smoothquant,
  author = {Guangxuan Xiao and Ji~Lin and Mickael Seznec and Hao Wu and Julien Demouth and Song Han},
  title = {Smoothquant: Accurate and efficient post-training quantization for large language models},
  booktitle = {Proc.~ICML},
  year = {2023}
}

@article{xiao2023bgeembedding,
  author = {Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighof},
  title = {C-pack: Packaged resources to advance general chinese embedding},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2309.07597}
}

@inproceedings{yang2018hotpotqa,
  author = {Zhilin Yang and Peng Qi and Saizheng Zhang and Yoshua Bengio and William~W. Cohen and Ruslan Salakhutdinov and Christopher~D. Manning},
  title = {HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  booktitle = {Proc.~EMNLP},
  year = {2018}
}

@inproceedings{yu2022orca,
  author = {Gyeong-In Yu and Joo~Seong Jeong and Geon-Woo Kim and Soojeong Kim and Byung-Gon Chun},
  title = {Orca: A distributed serving system for Transformer-Based generative models},
  booktitle = {Proc.~USENIX OSDI},
  year = {2022}
}

@article{zaharia2016apachespark,
  author = {Matei Zaharia and Reynold~S Xin and Patrick Wendell and Tathagata Das and Michael Armbrust and Ankur Dave and Xiangrui Meng and Josh Rosen and Shivaram Venkataraman and Michael~J Franklin and et~al.},
  title = {Apache spark: a unified engine for big data processing},
  journal = {Communications of the ACM},
  year = {2016}
}

@inproceedings{zhang2023shepherd,
  author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
  title = {SHEPHERD: Serving DNNs in the wild},
  booktitle = {Proc.~USENIX NSDI},
  year = {2023}
}

@article{zheng2023sglang,
  author = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Jeff Huang and Chuyue Sun and Cody~Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph~E Gonzalez and et~al.},
  title = {Efficiently programming large language models using sglang},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2312.07104}
}

@article{zhong2024distserve,
  author = {Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
  title = {Distserve: Disaggregating prefill and decoding for goodput-optimized large language model serving},
  journal = {arXiv},
  year = {2024},
  url = {https://arxiv.org/abs/2401.09670}
}

@article{zhu2023optimalcache,
  author = {Banghua Zhu and Ying Sheng and Lianmin Zheng and Clark Barrett and Michael~I Jordan and Jiantao Jiao},
  title = {On optimal caching and model multiplexing for large model inference},
  journal = {arXiv},
  year = {2023},
  url = {https://arxiv.org/abs/2306.02003}
}
```
























